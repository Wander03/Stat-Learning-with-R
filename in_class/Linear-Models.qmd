---
title: "Linear Models Review"
author: "Andrew Kerr"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    embed-resources: true
editor: source
---

## Setup

Declare your libraries:

```{r}
#| label: libraries-r
#| include: false
library(tidyverse)
library(tidymodels)
```

# The Dataset

Today we will use a dataset containing counts of babies born named "Allison" between 1997 and 2014. Run the chunk below to load the data.

```{r}
# You will need to change the path to load in your data! 

allisons_df <- read_csv(here::here("data", "allisons.csv"))
```

You can preview the dataset by clicking on the `allison_df` object in the upper right hand corner (of the Environment tab). Or you could type the following code into your console (`View(allison_df)`).

## Inspecting the Data

The first thing we need to do with any dataset is check for missing data, and make sure the variables are the right type. An easy way to do this is to use the `summary()` function which provides a summary of each variable in the dataset:

```{r}
allisons_df %>% 
  summary()
```

Based on the output, it looks like `Year` and `Count` are the correct data type we want (numerical)!

## Visualizing the Data

The next thing we should do is visualize our variables to get a feel for what is going on in this data.

```{r}
allisons_df %>%
  ggplot(aes(x = Year, y = Count)) +
  geom_jitter()
```

Hmmm.... clearly there is some kind of linear-ish decreasing trend in number of Allisons, but 2008 and 2009 don't seem to fit this trend...

## Fitting a model

Our first step is to establish a which model(s) we want to try on the data.

For now, this is just a simple linear model.

To establish the model, we need to determine which R package it comes from (the "engine") and whether we are doing *regression* or *classification*.

(These functions come from the *tidymodels* package that we loaded in the setup chunk.)

```{r}
lin_reg <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")
```

Next, we will **fit** the model to our data:

```{r}
lin_reg_fit <- lin_reg %>%
  fit(Count ~ Year, data = allisons_df)
```

Let's check out the output of this model fit:

```{r}
lin_reg_fit %>% 
  broom::tidy()
```

How do we interpret this?

-   The slope is -102. That means there are about 102 fewer babies named Allison born each year since 1997!

-   The intercept is 209,815 which means the model estimates that nearly 210,000 Allisons would have been born in year 0 (AD). That seems like an interesting prediction...

```{r}
lin_reg_fit %>% 
  broom::glance()
```

-   The p-value of the model is 0.000217. That means it is very likely that there is a relationship between these variables.

-   The r-squared value is 0.585. This means 58.5% of the variance in Allisons over the years is explained by the year the baby was born.

## Residuals

Now let's look the residuals of the model.

First, we can find out what values were predicted by the model using the `predict()` function. This function takes two inputs:

1. the model object to use when predicting
2. a dataset to use for prediction

```{r}
predict(lin_reg_fit, new_data = allisons_df)
```

When you run this code you should get a tibble of predictions, with one column named `.pred`. We need to extract this column from the dataframe in order for us to be able to add the column to our `allisons_df` dataframe. 

We can do this using the following code:

```{r}
predict(lin_reg_fit, new_data = allisons_df)$.pred
```

That gives us a vector which is easily added to our dataframe in a `mutate()` step. So this is how we put this all together:

```{r}
allisons_df <- allisons_df %>%
  mutate(
    Prediction = predict(lin_reg_fit, new_data = allisons_df)$.pred
         ) 
```

Then, we can calculate and visualize the residuals:

```{r}
# This could have been done in the SAME mutate() as the Predictions, but I chose to break it apart
allisons_df <- allisons_df %>% 
    mutate(
    residuals = Count - Prediction
  )

allisons_df %>%
  ggplot(aes(x = Year, y = residuals)) +
    geom_point()
```

Do the residuals seem to represent "random noise"?

That is, was our choice of model reasonable?

- No, they do not look like random noise. You should try a different model!

## Metrics

If we are trying to find the "best" model, we should measure how well this one did.

We can compute the SSE and MSE "by hand" using our `residuals` column:

```{r}
sum(allisons_df$residuals^2)
mean(allisons_df$residuals^2)
```

Alternatively, if we hadn't saved our residuals in their own column (like you might if you have residuals from multiple models), you could take the following approach to calculate the RMSE:

```{r}
allisons_df %>%
  rmse(truth = Count,
      estimate = Prediction)
```

