---
title: "Classification: LDA and QDA"
author: "Andrew Kerr"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: source
embed-resources: true
---

```{r}
#| label: libraries-r
#| include: false
library(tidyverse)
library(tidymodels)
library(glmnet)
library(discrim)
library(GGally)
library(car)

set.seed(42069)
```

# Part Zero

```{r}
#| label: data-read
#| message: false

ha <- read_csv(here::here('data', 'heart_attack.csv'))
```

```{r}
#| label: data-cleaning

ha_clean <- ha %>%
  mutate(
    across(.cols = c(sex, cp, restecg), 
           .fns = ~as.factor(.x)
           ), 
    output = factor(output, levels = c(1, 0)
                    ) # make "at-risk" the primary target
  ) %>%
  drop_na(output)
```

```{r}
#| label: EDA
#| message: false

# check for class imbalance
ha_clean %>%
  group_by(output) %>%
  count()

ggpairs(ha_clean, columns = c("age", "trtbps", "chol", "thalach", "output"),
        aes(color = output))

ggpairs(ha_clean, columns = c("sex", "cp", "restecg", "output"),
        aes(color = output))

ggpairs(ha_clean,
        aes(color = output))
```

# Part One: Fitting Models
#### Set Up

```{r}
#| label: model-fit-function

fit_model <- function(data, data_cv, mod, rec, rec_name) {
  
  rec_wflow <- workflow() %>%
    add_recipe(rec) %>%
    add_model(mod)
  
  rec_fit <- rec_wflow %>% fit(data)
  rec_fit_cv <- rec_wflow %>% fit_resamples(data_cv)
  
  # if(rec_name == 'rec_main' & mod$engine == 'glm') cat('VIF:', vif(extract_fit_engine(rec_fit)), '\n')
  
  roc_auc <- collect_metrics(rec_fit_cv) %>% filter(.metric == 'roc_auc') %>% pull(mean)

  pred_output <- data %>%
    mutate(predicted_response = predict(rec_fit, .)$.pred_class) 
  
  conf_matrix <- pred_output %>%
    conf_mat(truth = output, estimate = predicted_response) %>%
    tidy() %>%
    pivot_wider(names_from = name, values_from = value)
  
  tp <- conf_matrix$cell_1_1[1]
  fp <- conf_matrix$cell_1_2[1]
  fn <- conf_matrix$cell_2_1[1]
  tn <- conf_matrix$cell_2_2[1]

  data.frame(
    Recipe = rec_name,
    ROC_AUC = roc_auc,
    TN = tn,
    FP = fp,
    FN = fn,
    TP = tp,
    Precision = precision(pred_output, truth = output, estimate = predicted_response)$.estimate,
    Recall = recall(pred_output, truth = output, estimate = predicted_response)$.estimate
  )
  
}
```

```{r}
#| label: knn-k-function

find_k <- function(k_grid, data_cv, mod, rec, rec_name) {
  
  rec_wflow <- workflow() %>%
    add_recipe(rec) %>%
    add_model(mod)
  
  knn_grid_search <-
    tune_grid(
      rec_wflow,
      resamples = data_cv,
      grid = k_grid
    )

  metrics <- collect_metrics(knn_grid_search) %>% 
    filter(.metric == 'roc_auc')

  data.frame(
    Recipe = rec_name,
    ROC_AUC = metrics$mean,
    k = metrics$neighbors
  )
  
}
```

```{r}
#| label: starting-recipes

rec_main <- recipe(output ~ ., data = ha_clean) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

rec_age <- recipe(output ~ age, data = ha_clean) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

rec_thalach <- recipe(output ~ thalach, data = ha_clean) %>%
  step_normalize(all_numeric_predictors())

rec_trtbps <- recipe(output ~ trtbps, data = ha_clean) %>%
  step_normalize(all_numeric_predictors())

rec_chol <- recipe(output ~ chol, data = ha_clean) %>%
  step_normalize(all_numeric_predictors())

rec_sex <- recipe(output ~ sex, data = ha_clean) %>%
  step_dummy(all_nominal_predictors())

rec_restecg <- recipe(output ~ restecg, data = ha_clean) %>%
  step_dummy(all_nominal_predictors())

rec_cp <- recipe(output ~ cp, data = ha_clean) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

base_recipes <- list(
  rec_main = rec_main,
  rec_age = rec_age,
  rec_thalach = rec_thalach,
  rec_trtbps = rec_trtbps,
  rec_chol = rec_chol,
  rec_sex = rec_sex,
  rec_restecg = rec_restecg,
  rec_cp = rec_cp
)
```

```{r}
#| label: added-recipes

rec_1 <- recipe(output ~ cp + thalach, data = ha_clean) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

rec_2 <- rec_1 %>%
  step_interact(terms = ~ thalach:starts_with("cp_"))

rec_3 <- recipe(output ~ cp + thalach + age, data = ha_clean) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

rec_4 <- rec_3 %>%
  step_interact(terms = ~ age:thalach)

rec_5 <- rec_3 %>%
  step_interact(terms = ~ age:starts_with("cp_"))

rec_6 <- rec_main %>%
  step_interact(terms = ~ thalach:starts_with("cp_"))

rec_7 <- rec_main %>%
  step_interact(terms = ~ age:thalach)

rec_8 <- rec_main %>%
  step_interact(terms = ~ thalach:starts_with("cp_") + age:thalach)

test_recipes <- append(
  base_recipes,
  list(
    rec_1 = rec_1,
    rec_2 = rec_2,
    rec_3 = rec_3,
    rec_4 = rec_4,
    rec_5 = rec_5,
    rec_6 = rec_6,
    rec_7 = rec_7,
    rec_8 = rec_8
  )
)
```

#### KNN

```{r}
#| label: KNN-model-fitting

ha_cv <- vfold_cv(ha_clean, v = 10)

k_grid <- grid_regular(neighbors(c(1,100)), 
                       levels = 50)

knn_mod_tune <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

results_knn_k <- list_rbind(map2(test_recipes, names(test_recipes), ~find_k(k_grid, ha_cv, knn_mod_tune, .x, .y)))

best_k <- results_knn_k %>%
  group_by(Recipe) %>%
  slice(which.max(ROC_AUC)) %>%
  ungroup()

# results_knn_k %>%
#   filter(Recipe == 'rec_main') %>%
#   slice_max(ROC_AUC, n = 10)

# results_knn_k %>%
#   filter(Recipe == 'rec_2') %>%
#   slice_max(ROC_AUC, n = 3)

results_knn <- list_rbind(map2(test_recipes, names(test_recipes), ~{
  
  cur_k <- best_k %>% filter(Recipe == .y) %>% pull(k)
  
  knn_mod <- nearest_neighbor(neighbors = cur_k) %>%
  set_engine("kknn") %>%
  set_mode("classification")
  
  fit_model(ha_clean, ha_cv, knn_mod, .x, .y) %>% mutate(k = cur_k)
  
  }))

results_knn %>% slice_max(ROC_AUC, n = 5)
```

```{r}
#| label: best-KNN

knn_mod <- nearest_neighbor(neighbors = 59) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_wflow <- workflow() %>%
    add_recipe(rec_main) %>%
    add_model(knn_mod)
  
best_knn <- knn_wflow %>% fit(ha_clean)
```

To tune k, I did grid search from k = 1 to k = 100 with 50 levels. From the output, I selected the k with the greatest ROC AUC value and used this as my best k.

#### Logistic Regression

```{r}
#| label: LR-model-fitting

logit_mod <- logistic_reg() %>%
  set_mode("classification") %>%
  set_engine("glm")

results_logreg <- list_rbind(map2(test_recipes, names(test_recipes), ~fit_model(ha_clean, ha_cv, logit_mod, .x, .y)))

results_logreg %>% slice_max(ROC_AUC, n = 5)
```

```{r}
#| label: best-LR

logreg_wflow <- workflow() %>%
    add_recipe(rec_7) %>%
    add_model(logit_mod)
  
best_logreg <- logreg_wflow %>% fit(ha_clean)
```

#### LDA

```{r}
#| label: LDA-model-fitting

lda_mod <- discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification")

results_lda <- list_rbind(map2(test_recipes, names(test_recipes), ~fit_model(ha_clean, ha_cv, lda_mod, .x, .y)))

results_lda %>% slice_max(ROC_AUC, n = 5)
```

```{r}
#| label: best-LDA

lda_wflow <- workflow() %>%
    add_recipe(rec_7) %>%
    add_model(lda_mod)
  
best_lda <- lda_wflow %>% fit(ha_clean)
```

#### QDA

```{r}
#| label: QDA-model-fitting

qda_mod <- discrim_regularized(frac_common_cov = 0) %>% 
  set_engine('klaR') %>% 
  set_mode('classification')

results_qda <- list_rbind(map2(test_recipes, names(test_recipes), ~fit_model(ha_clean, ha_cv, qda_mod, .x, .y)))

results_qda %>% slice_max(ROC_AUC, n = 5)
```

```{r}
#| label: best-QDA

qda_wflow <- workflow() %>%
    add_recipe(rec_main) %>%
    add_model(qda_mod)
  
best_qda <- qda_wflow %>% fit(ha_clean)
```

# Part Two: Interpreting Models

#### Interpretation

```{r}
best_knn %>% extract_fit_parsnip()
best_logreg %>% extract_fit_parsnip()
best_lda %>% extract_fit_parsnip()
best_qda %>% extract_fit_parsnip()
```

The most important predictors to predicting heart attack risk are thalach, cp, and age as models only using these as predictors resulted in the greatest ROC AUC. This was consistent across all models.

#### ROC Curve

```{r}
#| label: ROC-plot-function

plot_roc_auc <- function(data, fit_mod) {
  
data %>%
  mutate(
    preds = predict(fit_mod, data, type = "prob")$.pred_1
  ) %>%
  roc_curve(
    truth = output,
    preds
  ) %>%
  autoplot()
  
}
```

```{r}
#| label: ROC-plots

best_mods <- list(best_knn, best_logreg, best_lda, best_qda)

map(best_mods, ~plot_roc_auc(ha_clean, .))
```


# Part Three: Metrics

Consider the following metrics:

-   **True Positive Rate** or **Sensitivity** = Of the observations that are truly Class A, how many were predicted to be Class A?

-   **Precision** or **Positive Predictive Value** = Of all the observations classified as Class A, how many of them were truly from Class A?

-   **True Negative Rate** or **Specificity** = Of all the observations classified as NOT Class A, how many were truly NOT Class A?

Compute each of these metrics (cross-validated) for your four models in Part One.

#### KNN

```{r}
#| label: KNN-metrics

knn_wflow %>%
  fit_resamples(ha_cv,
                metrics = metric_set(sensitivity, specificity, ppv)) %>%
  collect_metrics()
```

#### Logistic Regression

```{r}
#| label: LR-metrics

logreg_wflow %>%
  fit_resamples(ha_cv,
                metrics = metric_set(sensitivity, specificity, ppv)) %>%
  collect_metrics()
```

#### LDA

```{r}
#| label: LDA-metrics

lda_wflow %>%
  fit_resamples(ha_cv,
                metrics = metric_set(sensitivity, specificity, ppv)) %>%
  collect_metrics()
```

#### QDA

```{r}
#| label: QDA-metrics

qda_wflow %>%
  fit_resamples(ha_cv,
                metrics = metric_set(sensitivity, specificity, ppv)) %>%
  collect_metrics()
```

# Part Four: Validation

```{r}
#| label: data-cleaning-validation
#| message: false

ha_val <- read_csv(here::here('data', 'heart_attack_validation.csv'))

ha_val_clean <- ha_val %>%
  mutate(
    across(.cols = c(sex, cp, restecg), 
           .fns = ~as.factor(.x)
           ), 
    output = factor(output, levels = c(1, 0)
                    ) # make "at-risk" the primary target
  ) %>%
  drop_na(output)
```

Use each of your final models in Part One to:

-   predict the `output` variable in the validation dataset
-   output a confusion matrix
-   report the `roc.auc`, the `precision`, and the `recall`

Compare these values to the cross-validated estimates you reported in Part One. Did our measure of model success turn out to be approximately correct for the validation data?

#### KNN

```{r}
#| label: validation-KNN

ha_val_clean <- ha_val_clean %>%
  mutate(
    pred_knn = predict(best_knn, new_data = ha_val_clean)$.pred_class,
    proba_knn = predict(best_knn, new_data = ha_val_clean, type = 'prob')$.pred_1
    )

ha_val_clean %>%
  conf_mat(truth = output, estimate = pred_knn)

roc_auc_knn <- ha_val_clean %>%
  roc_auc(
    truth = output,
    proba_knn
    )

precision_knn <- ha_val_clean %>%
  precision(
    truth = output,
    estimate = pred_knn
  )

recall_knn <- ha_val_clean %>%
  recall(
    truth = output,
    estimate = pred_knn
  )

rbind(roc_auc_knn, precision_knn, recall_knn)
results_knn %>% slice_max(ROC_AUC)
```

My ROC AUC value for the validated data is slightly higher than it was for the CV data. The precision is higher for the validation data, but the recall is greater for the cross-validated data.

#### Logistic Regression

```{r}
#| label: validation-LR

ha_val_clean <- ha_val_clean %>%
  mutate(
    pred_logreg = predict(best_logreg, new_data = ha_val_clean)$.pred_class,
    proba_logreg = predict(best_logreg, new_data = ha_val_clean, type = 'prob')$.pred_1
    )

ha_val_clean %>%
  conf_mat(truth = output, estimate = pred_logreg)

roc_auc_logreg <- ha_val_clean %>%
  roc_auc(
    truth = output,
    proba_logreg
    )

precision_logreg <- ha_val_clean %>%
  precision(
    truth = output,
    estimate = pred_logreg
  )

recall_logreg <- ha_val_clean %>%
  recall(
    truth = output,
    estimate = pred_logreg
  )

rbind(roc_auc_logreg, precision_logreg, recall_logreg)
results_logreg %>% slice_max(ROC_AUC)
```

The ROC AUC value for the validation data is greater than it was in the cross-validated data, and the precision is slightly higher with the recall being slightly lower. 

#### LDA

```{r}
#| label: validation-LDA

ha_val_clean <- ha_val_clean %>%
  mutate(
    pred_lda = predict(best_lda, new_data = ha_val_clean)$.pred_class,
    proba_lda = predict(best_lda, new_data = ha_val_clean, type = 'prob')$.pred_1
    )

ha_val_clean %>%
  conf_mat(truth = output, estimate = pred_lda)

roc_auc_lda <- ha_val_clean %>%
  roc_auc(
    truth = output,
    proba_lda
    )

precision_lda <- ha_val_clean %>%
  precision(
    truth = output,
    estimate = pred_lda
  )

recall_lda <- ha_val_clean %>%
  recall(
    truth = output,
    estimate = pred_lda
  )

rbind(roc_auc_lda, precision_lda, recall_lda)
results_lda %>% slice_max(ROC_AUC)
```

The ROC AUC value for the validation data is greater than it was in the cross-validated data, and the precision is slightly higher with the recall being slightly lower.

#### QDA

```{r}
#| label: validation-QDA

ha_val_clean <- ha_val_clean %>%
  mutate(
    pred_qda = predict(best_qda, new_data = ha_val_clean)$.pred_class,
    proba_qda = predict(best_qda, new_data = ha_val_clean, type = 'prob')$.pred_1
    )

ha_val_clean %>%
  conf_mat(truth = output, estimate = pred_qda)

roc_auc_qda <- ha_val_clean %>%
  roc_auc(
    truth = output,
    proba_qda
    )

precision_qda <- ha_val_clean %>%
  precision(
    truth = output,
    estimate = pred_qda
  )

recall_qda <- ha_val_clean %>%
  recall(
    truth = output,
    estimate = pred_qda
  )

rbind(roc_auc_qda, precision_qda, recall_qda)
results_qda %>% slice_max(ROC_AUC)
```

The ROC AUC value for the validation data is greater than it was in the cross-validated data, and the precision is slightly higher with the recall being slightly lower.

# Part Four: Discussion

#### Q1

The hospital faces severe lawsuits if they deem a patient to be low risk, and that patient later experiences a heart attack.

We want to minimize false negatives, so we should use recall/sensitivity in model selection.

I would recommend QDA since this model has the highest value for recall on the validated data above.

#### Q2

The hospital is overfull, and wants to only use bed space for patients most in need of monitoring due to heart attack risk.

We want to maximize true positives and minimize false positives to save room for those who truly need it, so we would use precision.

I would recommend LDA because this model maximizes the separation between classes, which is what we want to do here.

#### Q3

The hospital is studying root causes of heart attacks, and would like to understand which biological measures are associated with heart attack risk.

We want to prioritize interpretability and feature importance, so we should use ROC AUC and accuracy to ensure that our model performs well and is using the most important features.

I would recommend logistic regression since this model is the most interpretable model since it provides coefficients.

#### Q4

The hospital is training a new batch of doctors, and they would like to compare the diagnoses of these doctors to the predictions given by the algorithm to measure the ability of new doctors to diagnose patients.

We want an overall robust model that is generally good, so we should use accuracy since we are testing to see if the new doctors are correct.

I would recommend KNN since its algorithm works similar to how a human thinks, classifying cases based off of similar cases. 
