---
title: "Lab 5: Variable Selection and Regularization"
author: "YOUR NAME HERE"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: source
execute: 
  cache: true
---

```{r}
#| label: libraries-r
#| include: false
library(tidyverse)
library(tidymodels)
library(glmnet)
library(discrim)
library(rpart)
library(rpart.plot)
library(baguette)
```

# Dataset: Gene Expression

Technology for measuring gene expression from human blood samples first became generally available to scientists in the 1990s. It was quickly found that this information was extremely powerful for predicting and categorizing diseases in humans, especially cancers.

At the time, it was very costly to process even a single sample. Thus, it was common to have datasets with large numbers of variables (genes) but very small numbers of samples (people). This data format went against all of classical statistics, which tends to assume many samples on a few variables. The field had to adapt accordingly, leading to methods like LASSO Regression.

Historical interlude: This area of study in statistics was called "High Dimension, Low Sample Size". Nowadays, the technology is much cheaper, and we often have very large sample sizes as well as very large variable sizes - another new problem for the early to mid 2000s that we called"Big Data". You can also read the original LASSO paper from 1996 here, if you're interested: https://statweb.stanford.edu/\~tibs/lasso/lasso.pdf. Fun fact: it's the second-most cited paper in all of statistics!

This lab's data is taken from a paper in 1999, which used clustering and classification techniques to classify patients between two types of cancer: acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).

You can read the paper here (although you don't need to for this lab): https://webdocs.cs.ualberta.ca/\~rgreiner/R/OLD-BiCluster/Golub_et_al_1999.pdf

In this lab, we'll see if we can learn more about the genetic drivers of AML and ALL via modern statistical learning techniques.

```{r}
#| label: data-load
#| message: false
#| echo: false

genes <- read_csv(here::here()
                  ) 
genes_validation <- read_csv(here::here()
                             )
```

## Reducing Size of Data

```{r}
#| label: subsetting-data

set.seed(282)

random_columns <- sample(3:ncol(genes), 
                         size = 100)
genes_sub <- genes %>% 
  select(1, 
         2, 
         all_of(random_columns)
         )
```

# Part One: Classification without regularization

#### Q1: Decision Tree

Fit a decision tree to this data. Which genes does it designate as most important for differentiating between ALL and AML cancers? How pure are the nodes?

#### Q2: Validation

Use your tree to predict on the validation set. How did it do?

#### Q3: Explanation

Give an intuitive explanation for why the above occurred.

That is to say: Your tree should have only one split, so it can't be too overfit. It should also have very good node purity, suggesting good predictive power. But the accuracy in Q2 is not as high as the node purity would suggest. Why is this?

#### Q4: Random Forest

Now fit a Random Forest to the data. 

#### Q5: Validation

Use your random forest to predict on the validation set. How did it do? 

#### Q6: Explanation

How does this method compare to a single decision tree? Give an explanation for the difference in results.

# Part Two: Variable Selection

#### Q7: Stepwise Selection

Use forwards or backwards selection (your choice) to choose the ideal number of variables, up to a maximum of 10. Which genes are chosen?

#### Q8: Tuning LASSO

Tune a LASSO regression. Identify the largest `penalty` parameter that doesn't cause you to lose any prediction accuracy.

#### Q9: LASSO Variable Selection

Using the penalty chosen in Q2, fit a final LASSO model on the **full data** (that is, on `genes` not on `genes_sub`). What genes were selected?

# Part Three: Reducing Variance of Coefficients

#### Q10: Ordinary Logistic Regression

Randomly divide the observations in the dataset (in `genes_sub`) in half. Fit a logistic regression on the with no penalty term to each half.

Report the estimates for the first five listed predictors. How different were they between the two subsamples of the dataset?

#### Q11: Ridge regression - tuning.

Tune a logistic regression with ridge penalty on `genes_sub`. Once again, choose the largest penalty that does not noticeably decrease the accuracy.

#### Q12: Comparison

Fit a logistic regression with the penalty selected in Q2 on the two random halves of the dataset that you created in Q1.

Compare the estimates of the first five variables. How different were they?

#### Q13: Explanation

In your own words, give an explanation for what we saw in Q1 versus in Q3. Why were the results similar or different?

# Part Four: A final model

#### Q14: Tuning

Using `genes_sub`, tune both the penalty and the mixture term. Choose the penalty that is largest without losing accuracy.

#### Q15: Mixture Parameter

Interpret the selected `mixture` parameter in words.

#### Q16: Conclusion

Using the parameters you selected above, fit your model to the **full dataset**.

How many genes were selected? Which seem to be most important?

Report the performance of the model on the validation set.
