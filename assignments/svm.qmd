---
title: "Support Vector Machines"
author: "Andrew Kerr"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: source
execute:
  message: false
---

```{r}
#| label: libraries-r
#| include: false

library(tidyverse)
library(tidymodels)
library(glmnet)
library(discrim)
library(rpart)
library(rpart.plot)
library(baguette)
library(magrittr)
library(patchwork)
library(kableExtra)
```

# Data: Zoo animals

This week's dataset contains information about animals commonly found at a zoo. The data contains dummy variables for several features that an animal might have, such as:

-   Is the animal a predator?
-   Does the animal produce milk?
-   Is the animal aquatic?

There is also one quantitative variable, `legs` stating how many legs the animal has.

Run the following code to read the data and convert the predictors into matrix form:

```{r}
#| message: false
#| warning: false
#| label: data-load

zoo <- read_csv("https://www.dropbox.com/s/kg89g2y3tp6p9yh/zoo_final.csv?dl=1")

zoo_matrix <- zoo %>%
  select(-Class_Type, -animal_name) %>%
  as.matrix() 
```

# Part One: PCA Preprocessing

#### Q1: PCA

Apply a PCA transformation to the matrix version of the data. Interpret the results - which variables are most important in the variation of the observations? What do each of the first three PCs seem to represent?

```{r}
recipe_PCA_bake <- recipe(~.,
                     data = zoo_matrix) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_pca(all_predictors()) %>%
  prep()

pca_output <- recipe_PCA_bake %$%
  steps %>% 
  pluck(3) %$%
  res

pca_output
```

- PC1: animals with not a lot of hair that do not produce milk that lay eggs (Mammals)
- PC2: Land animals that breath air and do not have many teeth and are not predator and really do not have fins (anything on land vs. Fish)
- PC3: Animals without a backbone and feathers that do not have a tail (Invertebrates)

#### Q2: Choosing PCs

Look at the percent of variance explained by each PC. Make an argument for the number of PCs you believe are appropriate to include in your analysis.

```{r}
pca_output %$%
  sdev %>%
  {cumsum(.^2) / sum(.^2)}
```

8 PCs recover 90% of the variance from the original data, so I would keep 8.

#### Q3: New dataset

Since PCA is a data processing tool, we can instead apply it as part of a recipe in tidymodels. In the code below, we carry out this process. First, you are tasked with specifying the `recipe()` for the model you are considering. Next, we use `update_role()` to assign the animal names to be an "id" column, so that the models don't use that variable in the classification process. Finally, we use `step_pca()` to automatically include the PCA process in your data pipeline. The `threshold` argument will select the number of PCs necessary to reach the proportion of total variance you specify (e.g., 80%).

Adjust the code below to complete this recipe:

```{r}
#| label: recipe-including-pca

zoo <- zoo %>%
  mutate(
    Class_Type = factor(Class_Type)
  )

zoo_rec <- recipe(Class_Type ~ ., data = zoo) %>%
  update_role(animal_name, new_role = "id") %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_pca(all_numeric(), threshold = 0.9) 
```

The `prep()` step is then used to prepare by computing the PCs, and the `bake()` function is then used to make a new dataframe with the chosen PCs as columns.

```{r}
#| label: prep-and-bake-pca-recipe

zoo_trained <- zoo_rec %>% 
  prep(zoo)

zoo_pcs <- zoo_trained %>% 
  bake(zoo)
```

#### Q4: Explore

To verify that the above process worked:

-   plot your observations in the first two PC dimensions (PC1, PC2), colored by the animal type
-   plot your observations in PC2 and PC3, colored by animal type
-   comment on the plots (i.e., why are certain animal types grouped the way that they are?)

```{r}
plot1 <- zoo_pcs %>%
  ggplot(aes(x = PC2, y = PC1, color = Class_Type)) +
  geom_point() +
  theme_bw() +
  labs(title = "Animal Type in PC1 by PC2 and PC2 by PC3", color = "Animal Type") +
  theme(axis.title.x = element_blank())

plot2 <- zoo_pcs %>%
  ggplot(aes(x = PC2, y = PC3, color = Class_Type)) +
  geom_point() +
  theme_bw() +
  labs(color = "Animal Type") 

combined_plot <- plot1 + plot2 + 
  plot_layout(ncol = 1, guides = "collect") & 
  theme(legend.position = "right")

combined_plot
```

In the first plot, we can see separate groups for Mammals and Fish, while the other animal types overlap. On the other hand, the second plot shows a separate group for Birds and Invertebrate with the remaining animal types overlapping. This lines up with what we noted in Question 1 about what the first three PCs represent:

- PC1 representing Mammals
- PC2 representing Fish
- PC3 representing Invertebrate

So, these three animal types are more separated by these PCs than the others.

## Part Two: SVM

#### Q5: Linear

Create a Support Vector Classifier (aka, an SVM with a linear kernel) that classifies the `Class_Type` of an animal based on the first three PCs (PC1, PC2, PC3). Choose the `cost` that is largest without losing accuracy.

Report appropriate metrics of your classifier.

```{r}
#| warning: false
#| error: false

set.seed(123)

zoo_cvs <- vfold_cv(zoo, v = 10)

svm_grid <- grid_regular(cost(), levels = 50)

svm_linear_tune <- svm_linear(cost = tune(), margin = 0.5) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_wflow_tune <- workflow() %>%
  add_model(svm_linear_tune) %>% 
  add_recipe(zoo_rec)

svm_grid_search <-
  tune_grid(
    svm_wflow_tune,
    resamples = zoo_cvs,
    grid = svm_grid,
    metrics = metric_set(accuracy, roc_auc)
  )

svm_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  slice_max(mean, n = 5) %>%
  kable()

best_cost <- svm_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  slice_max(mean) %>%
  slice_max(cost) %>%
  pull(cost)

svm_grid_search %>% 
  collect_metrics() %>%
  filter(cost == best_cost) %>%
  kable()
```

- cost: 1.0732907
- accuracy: 0.94
- roc_auc: 0.9569444

#### Q6: SVM

Repeat Q1, this time for a full Support Vector Machine with a polynomial kernel. Choose the `degree` that is the smallest without losing accuracy. (*You may use the same `cost` you chose in Q5.*)

```{r}
#| warning: false
#| error: false

set.seed(123)

zoo_cvs <- vfold_cv(zoo, v = 10)

svm_poly_grid <- data.frame(degree = seq(1, 10, by = 1))

svm_poly_tune <- svm_poly(cost = best_cost,
                          degree = tune(),
                          margin = 0.5) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_poly_wflow_tune <- workflow() %>%
  add_model(svm_poly_tune) %>% 
  add_recipe(zoo_rec)

svm_poly_grid_search <-
  tune_grid(
    svm_poly_wflow_tune,
    resamples = zoo_cvs,
    grid = svm_poly_grid,
    metrics = metric_set(accuracy, roc_auc)
  )

svm_poly_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  slice_max(mean, n = 5) %>%
  kable()

best_degree <- svm_poly_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  slice_max(mean) %>%
  slice_max(degree) %>%
  pull(degree)

svm_poly_grid_search %>% 
  collect_metrics() %>%
  filter(degree == best_degree) %>%
  kable()
```

- degree: 1
- accuracy: 0.94
- roc_auc: 0.9580556

#### Q7: Interpretation

**In simple terms**, explain why your polynomial SVM had better accuracy than your ordinary linear one.

In my case it did not since the best degree was 1 (linear), but I would expect polynomial SVM to have better accuracy since it is more flexible, allowing the separation line to curve and better separate the classes.

## Part Three: Full Data Comparison

Recall that PCA has two purposes:

1.  Reduce the dimensionality for interpretability reasons.

2.  Remove "noise" that is in the lower PCs, for better prediction power.

In this lab, we mainly used PCA for Goal #1. It was easier to visualize our animal types in the first couple PC dimensions. But did it also help us in Goal #2?

Fit an SVM classifier (linear kernal) using the original data, rather than the PCA transformed / reduced version. Is this better or worse than the model fit in Q5?

```{r}
#| warning: false
#| error: false

set.seed(123)

zoo_rec_2 <- recipe(Class_Type ~ ., data = zoo) %>%
  update_role(animal_name, new_role = "id") %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric())

zoo_cvs <- vfold_cv(zoo, v = 10)

svm_grid <- grid_regular(cost(), levels = 50)

svm_linear_tune <- svm_linear(cost = tune(), margin = 0.5) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_wflow_tune <- workflow() %>%
  add_model(svm_linear_tune) %>% 
  add_recipe(zoo_rec_2)

svm_grid_search <-
  tune_grid(
    svm_wflow_tune,
    resamples = zoo_cvs,
    grid = svm_grid,
    metrics = metric_set(accuracy, roc_auc)
  )

svm_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  slice_max(mean, n = 5) %>%
  kable()

best_cost <- svm_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  slice_max(mean) %>%
  slice_max(cost) %>%
  pull(cost)

svm_grid_search %>% 
  collect_metrics() %>%
  filter(cost == best_cost) %>%
  kable()
```

Q5 Model:

- cost: 1.0732907
- accuracy: 0.94
- roc_auc: 0.9569444

Q8 Model:

- cost: 0.05502864
- accuracy: 0.9600000
- roc_auc: 0.9868981

This model has a better fit then the one using PCA. The accuracy is slightly higher, and so is the ROC AUC!
