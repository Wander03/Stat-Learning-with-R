---
title: "Midterm"
author: "Andrew Kerr"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
embed-resources: true
editor: source
---

```{r}
#| label: libraries-r
#| message: false

library(tidyverse)
library(tidymodels)
library(glmnet)
library(here)
library(janitor)
library(themis)
library(leaps)
library(discrim)
library(ranger)
library(dataMaid)
```

# Data

## Read-in

```{r}
#| label: data-read-in
#| message: false

mvb_train <- read_csv(here('data', 'midterm', 'm_v_b_train.csv')) %>% clean_names()
mvb_test <- read_csv(here('data', 'midterm', 'm_v_b_test.csv')) %>% clean_names()
```

## Cleaning

```{r}
#| label: data-cleaning

factor_cols <- c('gender',
                 'education_level',
                 'location',
                 'favorite_season',
                 'pets',
                 'environmental_concerns',
                 'preference'
                 )

mvb_train <- mvb_train %>%
  mutate(
    across(.cols = all_of(factor_cols), .fns = ~factor(.))
  )

mvb_test <- mvb_test %>%
  mutate(
    across(.cols = any_of(factor_cols), .fns = ~factor(.))
  )

# makeDataReport(mvb_train, output = 'html', replace = T)
```

# Functions

```{r}
#| label: prediction-function

predict_metrics <- function(pred_data, fitted_model) {
  
  genes_sub_pred <- pred_data %>%
    bind_cols(predict(fitted_model, pred_data, type = "prob")) %>%
    mutate(
      .pred_preference = predict(fitted_model, pred_data)$.pred_class
    ) %>%
    select(
      1,
      preference,
      .pred_0,
      .pred_1,
      .pred_preference
    )

  accuracy_tree_bag <- genes_sub_pred %>%
    accuracy(
      truth = preference,
      estimate = .pred_preference
    )
  
  precision_tree_bag <- genes_sub_pred %>%
    precision(
      truth = preference,
      estimate = .pred_preference
    )
  
  recall_tree_bag <- genes_sub_pred %>%
    recall(
      truth = preference,
      estimate = .pred_preference
    )
  
  roc_auc_tree_bag <- genes_sub_pred %>%
    roc_auc(
      truth = preference,
      .pred_0
      )

  gain_capture_tree_bag <- genes_sub_pred %>%
    gain_capture(
      truth = preference,
      .pred_0
      )

  rbind(accuracy_tree_bag, 
        roc_auc_tree_bag, 
        gain_capture_tree_bag, 
        precision_tree_bag, 
        recall_tree_bag
        )

}
```

```{r}
#| label: knn-k-function

find_k <- function(k_grid, data_cv, mod, rec, rec_name) {
  
  rec_wflow <- workflow() %>%
    add_recipe(rec) %>%
    add_model(mod)
  
  knn_grid_search <-
    tune_grid(
      rec_wflow,
      resamples = data_cv,
      grid = k_grid
    )

  metrics <- collect_metrics(knn_grid_search) %>% 
    filter(.metric == 'roc_auc')

  data.frame(
    Recipe = rec_name,
    ROC_AUC = metrics$mean,
    k = metrics$neighbors
  )
  
}
```

```{r}
#| label: model-fit-function

fit_model <- function(data, data_cv, mod, rec, rec_name) {
  
  rec_wflow <- workflow() %>%
    add_recipe(rec) %>%
    add_model(mod)
  
  rec_fit <- rec_wflow %>% fit(data)
  rec_fit_cv <- rec_wflow %>% fit_resamples(data_cv)
  
  roc_auc <- collect_metrics(rec_fit_cv) %>% filter(.metric == 'roc_auc') %>% pull(mean)

  pred_output <- data %>%
    mutate(predicted_response = predict(rec_fit, .)$.pred_class) 
  
  conf_matrix <- pred_output %>%
    conf_mat(truth = preference, estimate = predicted_response) %>%
    tidy() %>%
    pivot_wider(names_from = name, values_from = value)
  
  tp <- conf_matrix$cell_1_1[1]
  fp <- conf_matrix$cell_1_2[1]
  fn <- conf_matrix$cell_2_1[1]
  tn <- conf_matrix$cell_2_2[1]

  data.frame(
    Recipe = rec_name,
    ROC_AUC = roc_auc,
    TN = tn,
    FP = fp,
    FN = fn,
    TP = tp,
    Precision = precision(pred_output, truth = preference, estimate = predicted_response)$.estimate,
    Recall = recall(pred_output, truth = preference, estimate = predicted_response)$.estimate
  )
  
}
```

# Modeling

## Starting Recipes

```{r}
#| label: recipes

recipe_full <- recipe(preference ~ ., data = mvb_train) %>%
    update_role(id_num, new_role = "id variable")

recipe_full_norm <- recipe_full %>% step_normalize(all_numeric_predictors())

recipe_full_upsample <- recipe(preference ~ ., data = mvb_train) %>%
    update_role(id_num, new_role = "id variable") %>%
    step_upsample(preference)

recipe_full_downsample <- recipe(preference ~ ., data = mvb_train) %>%
    update_role(id_num, new_role = "id variable") %>%
    step_downsample(preference)
```

## Recipe Tuning

```{r}
#| lable: variable-selection

################################################################################

# Create all pairwise interaction terms
interaction_data <- model.matrix(~ .^2, data = mvb_train[,c(-1,-14)])

# Convert back to a data frame (remove the intercept column if present)
interaction_data <- cbind(mvb_train[,14], as.data.frame(interaction_data[,-1]))

# The above 2 lines of code are origionally generated from ChatGPT

################################################################################

models_forward <- regsubsets(preference ~ ., 
                             data = interaction_data, 
                             method = "forward", 
                             nvmax = 100)

forward_bics <- tibble(bic = summary(models_forward)$bic, 
                    model = seq(from = 1, 
                                to = 100, 
                                by = 1
                                )
                    )

min_bic <- slice_min(forward_bics, order_by = bic) %>%
  pull(model)

best_forward <- summary(models_forward)$outmat[min_bic, ]
names(best_forward[best_forward == "*"])

################################################################################

models_backward <- regsubsets(preference ~ ., 
                             data = interaction_data, 
                             method = "backward", 
                             nvmax = 100)

backward_bics <- tibble(bic = summary(models_backward)$bic, 
                    model = seq(from = 1, 
                                to = 100, 
                                by = 1
                                )
                    )

min_bic <- slice_min(backward_bics, order_by = bic) %>%
  pull(model)

best_backward <- summary(models_backward)$outmat[min_bic, ]
names(best_backward[best_backward == "*"])

################################################################################

models_subset <- regsubsets(preference ~ ., 
                     data = mvb_train[,-1], 
                     method = "exhaustive",
                     nvmax = 20)

subset_bics <- tibble(bic = summary(models_subset)$bic, 
                    model = seq(from = 1, 
                                to = 18, 
                                by = 1
                                )
                    )

min_bic <- slice_min(subset_bics, order_by = bic) %>%
  pull(model)

best_subset <- summary(models_subset)$outmat[min_bic, ]
names(best_subset[best_subset == "*"])

################################################################################

recipe_forward <- recipe(preference ~ 
                           proximity_to_mountains + 
                           proximity_to_beaches +
                           gender +
                           education_level,
                           data = mvb_train) %>%
  step_interact(terms = ~gender:education_level)

recipe_forward_norm <- recipe_forward %>% 
  step_normalize(all_numeric_predictors())

recipe_best_subset <- recipe(preference ~ 
                               proximity_to_mountains + 
                               proximity_to_beaches, 
                             data = mvb_train)

recipe_best_subset_norm <- recipe_best_subset %>% 
  step_normalize(all_numeric_predictors())
```

```{r}
#| label: recipe-list

test_recipes <- list(full = recipe_full_norm, 
                     best_subset = recipe_best_subset_norm,
                     best_forward = recipe_forward_norm)
```

## Cross Validation

```{r}
#| label: initialize-CV

set.seed(123)

mvb_cvs <- vfold_cv(mvb_train, v = 10)
```

## Random Forest Model
### Full Model

```{r}
#| lablel: RF-tune-full

rf_grid <- grid_regular(mtry(c(1, ncol(mvb_train) - 2)),
                        min_n(), 
                        levels = 10)

rf_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow_tune <- workflow() %>%
  add_model(rf_mod_tune) %>% 
  add_recipe(recipe_full)

rf_grid_search <-
  tune_grid(
    rf_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.8) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .83, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-full-2

rf_grid <- grid_regular(mtry(c(6, 12)),
                        min_n(c(15, 60)), 
                        levels = 10)

rf_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow_tune <- workflow() %>%
  add_model(rf_mod_tune) %>% 
  add_recipe(recipe_full)

rf_grid_search <-
  tune_grid(
    rf_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.8) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .83, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-full-3

rf_grid <- grid_regular(mtry(c(7, 12)),
                        min_n(c(35, 60)), 
                        levels = 25)

rf_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow_tune <- workflow() %>%
  add_model(rf_mod_tune) %>% 
  add_recipe(recipe_full)

rf_grid_search <-
  tune_grid(
    rf_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.83) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .85, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| label: RF-fitted

rf_mod <- rand_forest(mtry = 8, 
                      min_n = 39,
                      trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow <- workflow() %>%
  add_model(rf_mod) %>% 
  add_recipe(recipe_full)

rf_fit <- rf_wflow %>%
  fit(mvb_train)

predict_metrics(inner_join(mvb_test, full_df), rf_fit)

rf_tree_fit_extracted <- rf_fit %>%
  extract_fit_parsnip()

ranger_fit <- rf_tree_fit_extracted$fit

importance(ranger_fit) %>%
  data.frame(Feature = names(.), Importance = .) %>%
  ggplot(aes(x = Importance, y = reorder(Feature, -Importance))) +
  geom_col(fill = 'firebrick') +
  theme_bw() +
  labs(
    x = 'Variable Importance',
    y = 'Features',
    title = 'Random Forest Variable Importance'
  )
```

### Full Upsampled Model

```{r}
#| lablel: RF-tune-upsample

rf_up_grid <- grid_regular(mtry(c(1, ncol(mvb_train) - 2)),
                        min_n(),
                        levels = 10)

rf_up_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_up_wflow_tune <- workflow() %>%
  add_model(rf_up_mod_tune) %>% 
  add_recipe(recipe_full_upsample)

rf_up_grid_search <-
  tune_grid(
    rf_up_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_up_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 3)

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.82) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .84, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-upsample-2

rf_up_grid <- grid_regular(mtry(c(3, 10)),
                        min_n(c(25, 90)), 
                        levels = 10)

rf_up_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_up_wflow_tune <- workflow() %>%
  add_model(rf_up_mod_tune) %>% 
  add_recipe(recipe_full_upsample)

rf_up_grid_search <-
  tune_grid(
    rf_up_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_up_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 3)

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.82) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .84, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-upsample-3

rf_up_grid <- grid_regular(mtry(c(5, 11)),
                        min_n(c(65, 80)), 
                        levels = 25)

rf_up_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_up_wflow_tune <- workflow() %>%
  add_model(rf_up_mod_tune) %>% 
  add_recipe(recipe_full_upsample)

rf_up_grid_search <-
  tune_grid(
    rf_up_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_up_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 3)

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.82) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .84, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-upsample-trees

# rf_up_grid <- data.frame(trees = c(10, 100, 1000))
# rf_up_grid <- data.frame(trees = c(900, 1000, 1100))
rf_up_grid <- data.frame(trees = c(1100, 1200, 1300))

rf_up_mod_tune <- rand_forest(mtry = 6, 
                           min_n = 67,
                           trees = tune()) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_up_wflow_tune <- workflow() %>%
  add_model(rf_up_mod_tune) %>% 
  add_recipe(recipe_full_upsample)

rf_up_grid_search <-
  tune_grid(
    rf_up_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_up_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 3)
```

```{r}
#| label: RF-upsample-fitted

rf_mod <- rand_forest(mtry = 6, 
                      min_n = 67,
                      trees = 1200) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow <- workflow() %>%
  add_model(rf_mod) %>% 
  add_recipe(recipe_full_upsample)

rf_fit <- rf_wflow %>%
  fit(mvb_train)

predict_metrics(inner_join(mvb_test, full_df), rf_fit)

rf_tree_fit_extracted <- rf_fit %>%
  extract_fit_parsnip()

ranger_fit <- rf_tree_fit_extracted$fit

importance(ranger_fit) %>%
  data.frame(Feature = names(.), Importance = .) %>%
  ggplot(aes(x = Importance, y = reorder(Feature, -Importance))) +
  geom_col(fill = 'firebrick') +
  theme_bw() +
  labs(
    x = 'Variable Importance',
    y = 'Features',
    title = 'Random Forest Variable Importance'
  )
```

### Full Downsampled Model

```{r}
#| lablel: RF-tune-downsample

rf_down_grid <- grid_regular(mtry(c(1, ncol(mvb_train) - 2)),
                        min_n(), 
                        levels = 10)

rf_down_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_down_wflow_tune <- workflow() %>%
  add_model(rf_down_mod_tune) %>% 
  add_recipe(recipe_full_downsample)

rf_down_grid_search <-
  tune_grid(
    rf_down_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_down_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_down_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)

rf_down_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.82) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .84, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-downsample-2

rf_down_grid <- grid_regular(mtry(c(4, 9)),
                        min_n(c(30, 60)), 
                        levels = 10)

rf_down_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_down_wflow_tune <- workflow() %>%
  add_model(rf_down_mod_tune) %>% 
  add_recipe(recipe_full_downsample)

rf_down_grid_search <-
  tune_grid(
    rf_down_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_down_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_down_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)

rf_down_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.82) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .85, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

## Logistic Regression

```{r}
#| label: logreg-fit

logreg_mod <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

logreg_mod_wflow <- workflow() %>%
  add_model(logreg_mod) %>% 
  add_recipe(recipe_best_subset_norm)

logreg_fit <- logreg_mod_wflow %>%
  fit_resamples(mvb_cvs)

logreg_fit %>%
  collect_metrics()
```

## KNN

```{r}
#| label: KNN-tune

k_grid <- grid_regular(neighbors(c(1,100)), 
                       levels = 10)

knn_mod_tune <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

results_knn_k <- list_rbind(map2(test_recipes, names(test_recipes), ~find_k(k_grid, mvb_cvs, knn_mod_tune, .x, .y)))

results_knn_k %>%
  slice_max(ROC_AUC, by = Recipe, n = 3)
```

```{r}
#| label: KNN-tune-2

k_grid <- grid_regular(neighbors(c(15,40)), 
                       levels = 20)

knn_mod_tune <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

results_knn_k_2 <- find_k(k_grid, mvb_cvs, knn_mod_tune, recipe_best_subset_norm, 'best_subset')

results_knn_k_2 %>%
  slice_max(ROC_AUC, n = 5)
```

```{r}
#| label: KNN-tune-3

k_grid <- grid_regular(neighbors(c(20,35)), 
                       levels = 15)

knn_mod_tune <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

results_knn_k_3 <- find_k(k_grid, mvb_cvs, knn_mod_tune, recipe_best_subset_norm, 'best_subset')

results_knn_k_3 %>%
  slice_max(ROC_AUC, n = 3)
```

```{r}
#| label: KNN-fit

knn_mod <- nearest_neighbor(neighbors = 28) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_wflow <- workflow() %>%
  add_model(knn_mod) %>% 
  add_recipe(recipe_best_subset_norm)

knn_fit <- knn_wflow %>%
  fit(mvb_train)

predict_metrics(inner_join(mvb_test[,-1], full_df), knn_fit)
```

## Elastic Net

```{r}
#| label: elastic-net-best-subset-tune

net_grid <- grid_regular(penalty(),
                         mixture(),
                         levels = 10)

net_mod_tune <- logistic_reg(penalty = tune(), 
                             mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

net_wflow_tune <- workflow() %>%
  add_model(net_mod_tune) %>% 
  add_recipe(recipe_best_subset_norm)

net_grid_search <-
  tune_grid(
    net_wflow_tune,
    resamples = mvb_cvs,
    grid = net_grid,
    metrics = metric_set(accuracy, roc_auc, gain_capture, precision, recall)
  )

net_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)
```

```{r}
#| label: elastic-net-best-subset-tune-2

net_grid <- grid_regular(penalty(c(-2, 10), trans = log2_trans()),
                         mixture(),
                         levels = 20)

net_mod_tune <- logistic_reg(penalty = tune(), 
                             mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

net_wflow_tune <- workflow() %>%
  add_model(net_mod_tune) %>% 
  add_recipe(recipe_best_subset_norm)

net_grid_search <-
  tune_grid(
    net_wflow_tune,
    resamples = mvb_cvs,
    grid = net_grid,
    metrics = metric_set(accuracy, roc_auc, gain_capture, precision, recall)
  )

net_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)
```

```{r}
#| label: elastic-net-best-subset-fit

net_mod <- logistic_reg(penalty = 0.6000514, 
                        mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

net_wflow <- workflow() %>%
  add_model(net_mod) %>% 
  add_recipe(recipe_best_subset_norm)

net_fit <- net_wflow %>%
  fit(mvb_train)

predict_metrics(inner_join(mvb_test[,-1], full_df), net_fit)
```

## LDA

```{r}
#| label: LDA-tune

lda_mod <- discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification")

results_lda <- list_rbind(map2(test_recipes, names(test_recipes), ~fit_model(mvb_train, mvb_cvs, lda_mod, .x, .y)))

results_lda
```

## QDA

```{r}
#| label: QDA-tune

qda_mod <- discrim_regularized(frac_common_cov = 0) %>% 
  set_engine('klaR') %>% 
  set_mode('classification')

results_qda <- list_rbind(map2(test_recipes[-3], names(test_recipes[-3]), ~fit_model(mvb_train, mvb_cvs, qda_mod, .x, .y)))

results_qda
```

# Submitted Models

## Submission 1

```{r}
#| label: RF-upsample-S1

rf_mod <- rand_forest(mtry = 6, 
                      min_n = 67,
                      trees = 1200) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow <- workflow() %>%
  add_model(rf_mod) %>% 
  add_recipe(recipe_full_upsample)

rf_fit <- rf_wflow %>%
  fit(mvb_train)

results <- mvb_test %>%
  mutate(
    ID_Num = id_num,
    Preference = predict(rf_fit, mvb_test, type = "prob")$.pred_1
    ) %>%
  select(ID_Num, Preference)

write_csv(results, 'submission1.csv')

predict_metrics(inner_join(mvb_test, full_df), rf_fit)

rf_tree_fit_extracted <- rf_fit %>%
  extract_fit_parsnip()

ranger_fit <- rf_tree_fit_extracted$fit

importance(ranger_fit) %>%
  data.frame(Feature = names(.), Importance = .) %>%
  ggplot(aes(x = Importance, y = reorder(Feature, -Importance))) +
  geom_col(fill = 'firebrick') +
  theme_bw() +
  labs(
    x = 'Variable Importance',
    y = 'Features',
    title = 'Random Forest Variable Importance'
  )
```




# REMOVE LATER

```{r}
full_df <- read.csv(here('data', 'midterm', 'mountains_vs_beaches_preferences.csv')) %>% clean_names()

full_df <- full_df %>%
  mutate(
    across(.cols = all_of(factor_cols), .fns = ~factor(.))
  ) %>%
  select(-preferred_activities)

```

```{r}
identical(inner_join(mvb_train[,-1], full_df), mvb_train[,-1])
```

```{r}
mvb_test[,-1]
```

