---
title: "Midterm"
author: "Andrew Kerr"
format: 
  html:
    toc: true
    toc-title: "Outline"
    toc-depth: 3
    code-fold: true
    code-line-numbers: true
    code-tools: true
    theme:
      light: flatly
      dark: darkly
    default: dark
embed-resources: true
editor: source
eval: false
---

```{r}
#| label: libraries-r
#| message: false

library(tidyverse)
library(tidymodels)
library(glmnet)
library(here)
library(janitor)
library(themis)
library(leaps)
library(discrim)
library(ranger)
```

# Data

## Read-in

```{r}
#| label: data-read-in
#| message: false

mvb_train <- read_csv(here('data', 'midterm', 'm_v_b_train.csv')) %>% clean_names()
mvb_test <- read_csv(here('data', 'midterm', 'm_v_b_test.csv')) %>% clean_names()
```

## Cleaning

```{r}
#| label: data-cleaning
#| message: false
#| warning: false

factor_cols <- c('gender',
                 'education_level',
                 'location',
                 'favorite_season',
                 'pets',
                 'environmental_concerns',
                 'preference'
                 )

mvb_train <- mvb_train %>%
  mutate(
    across(.cols = all_of(factor_cols), .fns = ~factor(.))
  )

mvb_test <- mvb_test %>%
  mutate(
    across(.cols = any_of(factor_cols), .fns = ~factor(.))
  )

mvb_train_testing <- mvb_train %>%
  mutate(m2 = proximity_to_mountains**2,
         b2 = proximity_to_beaches**2,
         m_log = log10(proximity_to_mountains),
         b_log = log10(proximity_to_beaches),
         m_sqrt = sqrt(proximity_to_mountains),
         b_sqrt = sqrt(proximity_to_beaches),
         t = m_log * b2,
         proxy_m_fac = factor(case_when(
           proximity_to_mountains < 100 ~ 'close',
           proximity_to_mountains < 200 ~ 'kinda close',
           TRUE ~ 'far'
         )),
         proxy_b_fac = factor(case_when(
           proximity_to_beaches < 100 ~ 'close',
           proximity_to_beaches < 200 ~ 'kinda close',
           TRUE ~ 'far'
         ))
         )

GGally::ggpairs(mvb_train, 
                columns = c("age", "income", "travel_frequency", 
                            "vacation_budget", "proximity_to_mountains", 
                            "proximity_to_beaches", "preference"),
                aes(color = preference)
                )

GGally::ggpairs(mvb_train, 
                columns = c("gender", "education_level", "location", 
                            "favorite_season", "pets", "environmental_concerns", 
                            "preference"),
                aes(color = preference)
                )

GGally::ggpairs(mvb_train_testing, 
                columns = c(
                  "proximity_to_mountains", "proximity_to_beaches", 
                  "education_level", "gender", "proxy_m_fac", "proxy_b_fac",
                  "preference"
                  ),
                aes(color = preference)
                )

GGally::ggpairs(mvb_train_testing, 
                columns = c(
                  "m2", "b2", "m_log", "b_log", "m_sqrt", "b_sqrt", "t", "preference"
                  ),
                aes(color = preference)
                )
```

# REMOVE LATER

```{r}
full_df <- read.csv(here('data', 'midterm', 'mountains_vs_beaches_preferences.csv')) %>% clean_names()

full_df <- full_df %>%
  mutate(
    across(.cols = all_of(factor_cols), .fns = ~factor(.))
  ) %>%
  select(-preferred_activities)

full_test <- inner_join(mvb_test, full_df)

full_test_2 <- full_test %>%
  mutate(m2 = proximity_to_mountains**2,
         b2 = proximity_to_beaches**2,
         m_log = log10(proximity_to_mountains),
         b_log = log10(proximity_to_beaches),
         m_sqrt = sqrt(proximity_to_mountains),
         b_sqrt = sqrt(proximity_to_beaches),
         t = m_log * b2,
         proxy_m_fac = factor(case_when(
           proximity_to_mountains < 100 ~ 'close',
           proximity_to_mountains < 200 ~ 'kinda close',
           TRUE ~ 'far'
         )),
         proxy_b_fac = factor(case_when(
           proximity_to_beaches < 100 ~ 'close',
           proximity_to_beaches < 200 ~ 'kinda close',
           TRUE ~ 'far'
         ))
         )

```

```{r}
identical(inner_join(mvb_train[,-1], full_df), mvb_train[,-1])
```

# Functions

```{r}
#| label: prediction-function

predict_metrics <- function(pred_data, fitted_model) {
  
  genes_sub_pred <- pred_data %>%
    bind_cols(predict(fitted_model, pred_data, type = "prob")) %>%
    mutate(
      .pred_preference = predict(fitted_model, pred_data)$.pred_class
    )

  accuracy_tree_bag <- genes_sub_pred %>%
    accuracy(
      truth = preference,
      estimate = .pred_preference
    )
  
  precision_tree_bag <- genes_sub_pred %>%
    precision(
      truth = preference,
      estimate = .pred_preference
    )
  
  recall_tree_bag <- genes_sub_pred %>%
    recall(
      truth = preference,
      estimate = .pred_preference
    )
  
  roc_auc_tree_bag <- genes_sub_pred %>%
    roc_auc(
      truth = preference,
      .pred_0
      )

  gain_capture_tree_bag <- genes_sub_pred %>%
    gain_capture(
      truth = preference,
      .pred_0
      )

  rbind(accuracy_tree_bag, 
        roc_auc_tree_bag, 
        gain_capture_tree_bag, 
        precision_tree_bag, 
        recall_tree_bag
        )

}
```

```{r}
#| label: knn-k-function

find_k <- function(k_grid, data_cv, mod, rec, rec_name) {
  
  rec_wflow <- workflow() %>%
    add_recipe(rec) %>%
    add_model(mod)
  
  knn_grid_search <-
    tune_grid(
      rec_wflow,
      resamples = data_cv,
      grid = k_grid
    )

  metrics <- collect_metrics(knn_grid_search) %>% 
    filter(.metric == 'roc_auc')

  data.frame(
    Recipe = rec_name,
    ROC_AUC = metrics$mean,
    k = metrics$neighbors
  )
  
}
```

```{r}
#| label: model-fit-function

fit_model <- function(data, data_cv, mod, rec, rec_name) {
  
  rec_wflow <- workflow() %>%
    add_recipe(rec) %>%
    add_model(mod)
  
  rec_fit <- rec_wflow %>% fit(data)
  rec_fit_cv <- rec_wflow %>% fit_resamples(data_cv)
  
  roc_auc <- collect_metrics(rec_fit_cv) %>% filter(.metric == 'roc_auc') %>% pull(mean)

  pred_output <- data %>%
    mutate(predicted_response = predict(rec_fit, .)$.pred_class) 
  
  conf_matrix <- pred_output %>%
    conf_mat(truth = preference, estimate = predicted_response) %>%
    tidy() %>%
    pivot_wider(names_from = name, values_from = value)
  
  tp <- conf_matrix$cell_1_1[1]
  fp <- conf_matrix$cell_1_2[1]
  fn <- conf_matrix$cell_2_1[1]
  tn <- conf_matrix$cell_2_2[1]

  data.frame(
    Recipe = rec_name,
    ROC_AUC = roc_auc,
    TN = tn,
    FP = fp,
    FN = fn,
    TP = tp,
    Precision = precision(pred_output, truth = preference, estimate = predicted_response)$.estimate,
    Recall = recall(pred_output, truth = preference, estimate = predicted_response)$.estimate
  )
  
}
```

# Modeling

## Starting Recipes

```{r}
#| label: recipes

recipe_full <- recipe(preference ~ ., data = mvb_train) %>%
    update_role(id_num, new_role = "id variable")

recipe_full_norm <- recipe_full %>% step_normalize(all_numeric_predictors())

recipe_full_upsample <- recipe(preference ~ ., data = mvb_train) %>%
    update_role(id_num, new_role = "id variable") %>%
    step_upsample(preference, seed = 123)

recipe_full_downsample <- recipe(preference ~ ., data = mvb_train) %>%
    update_role(id_num, new_role = "id variable") %>%
    step_downsample(preference, seed = 123)
```

## Recipe Tuning

```{r}
#| label: variable-selection

################################################################################

# Create all pairwise interaction terms
interaction_data <- model.matrix(~ .^2, data = mvb_train[,c(-1,-14)])

# Convert back to a data frame (remove the intercept column if present)
interaction_data <- cbind(mvb_train[,14], as.data.frame(interaction_data[,-1]))

# The above 2 lines of code are originally generated from ChatGPT

################################################################################

models_forward <- regsubsets(preference ~ ., 
                             data = interaction_data, 
                             method = "forward", 
                             nvmax = 100)

forward_bics <- tibble(bic = summary(models_forward)$bic, 
                    model = seq(from = 1, 
                                to = 100, 
                                by = 1
                                )
                    )

min_bic <- slice_min(forward_bics, order_by = bic) %>%
  pull(model)

best_forward <- summary(models_forward)$outmat[min_bic, ]
names(best_forward[best_forward == "*"])

################################################################################

models_backward <- regsubsets(preference ~ ., 
                             data = interaction_data, 
                             method = "backward", 
                             nvmax = 100)

backward_bics <- tibble(bic = summary(models_backward)$bic, 
                    model = seq(from = 1, 
                                to = 100, 
                                by = 1
                                )
                    )

min_bic <- slice_min(backward_bics, order_by = bic) %>%
  pull(model)

best_backward <- summary(models_backward)$outmat[min_bic, ]
names(best_backward[best_backward == "*"])

################################################################################

models_subset <- regsubsets(preference ~ ., 
                     data = mvb_train[,-1], 
                     method = "exhaustive",
                     nvmax = 20)

subset_bics <- tibble(bic = summary(models_subset)$bic, 
                    model = seq(from = 1, 
                                to = 18, 
                                by = 1
                                )
                    )

min_bic <- slice_min(subset_bics, order_by = bic) %>%
  pull(model)

best_subset <- summary(models_subset)$outmat[min_bic, ]
names(best_subset[best_subset == "*"])

################################################################################

recipe_forward <- recipe(preference ~ 
                           proximity_to_mountains + 
                           proximity_to_beaches +
                           gender +
                           education_level,
                           data = mvb_train) %>%
  step_interact(terms = ~gender:education_level)

recipe_forward_norm <- recipe_forward %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

recipe_best_subset <- recipe(preference ~ 
                               proximity_to_mountains + 
                               proximity_to_beaches, 
                             data = mvb_train)

recipe_best_subset_norm <- recipe_best_subset %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

recipe_forward_true <- recipe(preference ~ 
                           proximity_to_mountains + 
                           proximity_to_beaches,
                           data = mvb_train) %>%
  step_interact(terms = ~gender:education_level)

################################################################################

recipe_1 <- recipe_best_subset %>%
  step_interact(term = ~proximity_to_mountains:proximity_to_beaches) %>%
  step_normalize(all_numeric_predictors())

recipe_forward_upsample <- recipe_forward_norm %>%
  step_dummy(all_nominal_predictors()) %>%
  step_upsample(preference, seed = 123)
```

```{r}
#| label: recipe-list

test_recipes <- list(full = recipe_full_norm, 
                     best_subset = recipe_best_subset_norm,
                     best_forward = recipe_forward_norm)

test_recipes_plus <- test_recipes %>%
  append(list(recipe_full_upsample = recipe_full_upsample %>% step_normalize(all_numeric_predictors())))
```

## Cross Validation

```{r}
#| label: initialize-CV

set.seed(123)

mvb_cvs <- vfold_cv(mvb_train, v = 10)

mvb_cvs <- vfold_cv(mvb_train_testing, v = 10)
```

## Random Forest Model
### Full Model

```{r}
#| lablel: RF-tune-full

rf_grid <- grid_regular(mtry(c(1, ncol(mvb_train) - 2)),
                        min_n(), 
                        levels = 10)

rf_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow_tune <- workflow() %>%
  add_model(rf_mod_tune) %>% 
  add_recipe(recipe_full)

rf_grid_search <-
  tune_grid(
    rf_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.8) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .83, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-full-2

rf_grid <- grid_regular(mtry(c(6, 12)),
                        min_n(c(15, 60)), 
                        levels = 10)

rf_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow_tune <- workflow() %>%
  add_model(rf_mod_tune) %>% 
  add_recipe(recipe_full)

rf_grid_search <-
  tune_grid(
    rf_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.8) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .83, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-full-3

rf_grid <- grid_regular(mtry(c(7, 12)),
                        min_n(c(35, 60)), 
                        levels = 25)

rf_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow_tune <- workflow() %>%
  add_model(rf_mod_tune) %>% 
  add_recipe(recipe_full)

rf_grid_search <-
  tune_grid(
    rf_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)

rf_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.83) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .85, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| label: RF-fitted

rf_mod <- rand_forest(mtry = 8, 
                      min_n = 39,
                      trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow <- workflow() %>%
  add_model(rf_mod) %>% 
  add_recipe(recipe_full)

rf_fit <- rf_wflow %>%
  fit(mvb_train)

rf_tree_fit_extracted <- rf_fit %>%
  extract_fit_parsnip()

ranger_fit <- rf_tree_fit_extracted$fit

importance(ranger_fit) %>%
  data.frame(Feature = names(.), Importance = .) %>%
  ggplot(aes(x = Importance, y = reorder(Feature, -Importance))) +
  geom_col(fill = 'firebrick') +
  theme_bw() +
  labs(
    x = 'Variable Importance',
    y = 'Features',
    title = 'Random Forest Variable Importance'
  )
```

### Full Upsampled Model

```{r}
#| lablel: RF-tune-upsample

rf_up_grid <- grid_regular(mtry(c(1, ncol(mvb_train) - 2)),
                        min_n(),
                        levels = 10)

rf_up_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_up_wflow_tune <- workflow() %>%
  add_model(rf_up_mod_tune) %>% 
  add_recipe(recipe_full_upsample)

rf_up_grid_search <-
  tune_grid(
    rf_up_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_up_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 3)

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.82) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .84, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-upsample-2

rf_up_grid <- grid_regular(mtry(c(3, 10)),
                        min_n(c(25, 90)), 
                        levels = 10)

rf_up_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_up_wflow_tune <- workflow() %>%
  add_model(rf_up_mod_tune) %>% 
  add_recipe(recipe_full_upsample)

rf_up_grid_search <-
  tune_grid(
    rf_up_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_up_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 3)

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.82) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .84, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-upsample-3

rf_up_grid <- grid_regular(mtry(c(5, 11)),
                        min_n(c(65, 80)), 
                        levels = 25)

rf_up_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_up_wflow_tune <- workflow() %>%
  add_model(rf_up_mod_tune) %>% 
  add_recipe(recipe_full_upsample)

rf_up_grid_search <-
  tune_grid(
    rf_up_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_up_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 3)

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.82) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .84, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-upsample-trees

# rf_up_grid <- data.frame(trees = c(10, 100, 1000))
# rf_up_grid <- data.frame(trees = c(900, 1000, 1100))
rf_up_grid <- data.frame(trees = c(1100, 1200, 1300))

rf_up_mod_tune <- rand_forest(mtry = 6, 
                           min_n = 67,
                           trees = tune()) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_up_wflow_tune <- workflow() %>%
  add_model(rf_up_mod_tune) %>% 
  add_recipe(recipe_full_upsample)

rf_up_grid_search <-
  tune_grid(
    rf_up_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_up_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_up_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 3)
```

```{r}
#| label: RF-upsample-fitted

rf_mod <- rand_forest(mtry = 6, 
                      min_n = 67,
                      trees = 1200) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow <- workflow() %>%
  add_model(rf_mod) %>% 
  add_recipe(recipe_full_upsample)

rf_fit <- rf_wflow %>%
  fit(mvb_train)

rf_tree_fit_extracted <- rf_fit %>%
  extract_fit_parsnip()

ranger_fit <- rf_tree_fit_extracted$fit

importance(ranger_fit) %>%
  data.frame(Feature = names(.), Importance = .) %>%
  ggplot(aes(x = Importance, y = reorder(Feature, -Importance))) +
  geom_col(fill = 'firebrick') +
  theme_bw() +
  labs(
    x = 'Variable Importance',
    y = 'Features',
    title = 'Random Forest Variable Importance'
  )
```

### Full Downsampled Model

```{r}
#| lablel: RF-tune-downsample

rf_down_grid <- grid_regular(mtry(c(1, ncol(mvb_train) - 2)),
                             min_n(),
                             levels = 10)

rf_down_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_down_wflow_tune <- workflow() %>%
  add_model(rf_down_mod_tune) %>% 
  add_recipe(recipe_full_downsample)

rf_down_grid_search <-
  tune_grid(
    rf_down_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_down_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_down_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)

rf_down_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.82) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .84, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

```{r}
#| lablel: RF-tune-downsample-2

rf_down_grid <- grid_regular(mtry(c(4, 9)),
                        min_n(c(30, 60)), 
                        levels = 10)

rf_down_mod_tune <- rand_forest(mtry = tune(), 
                           min_n = tune(),
                           trees = 10) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_down_wflow_tune <- workflow() %>%
  add_model(rf_down_mod_tune) %>% 
  add_recipe(recipe_full_downsample)

rf_down_grid_search <-
  tune_grid(
    rf_down_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_down_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

rf_down_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)

rf_down_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc", mean > 0.82) %>%
  ggplot() +
    geom_line(aes(x = min_n, y = mean, color = factor(mtry))) +
    geom_hline(yintercept = .85, color = 'firebrick', linetype = 'dashed') +
    theme_bw()
```

## Decision Tree

```{r}
#| label: dc-tune

tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          min_n(), 
                          levels = 5)

tree_mod_tune <- decision_tree(cost_complexity = tune(),
                               tree_depth = tune(),
                               min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_wflow_tune <- workflow() %>%
  add_model(tree_mod_tune) %>% 
  add_recipe(recipe_full)

tree_grid_search <-
  tune_grid(
    tree_wflow_tune,
    resamples = mvb_cvs,
    grid = tree_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

tree_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)
```

```{r}
#| label: dc-tune-2

tree_grid <- grid_regular(cost_complexity(c(-20, -9)),
                          tree_depth(c(10, 30)),
                          min_n(), 
                          levels = 10)

tree_mod_tune <- decision_tree(cost_complexity = tune(),
                               tree_depth = tune(),
                               min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_wflow_tune <- workflow() %>%
  add_model(tree_mod_tune) %>% 
  add_recipe(recipe_best_subset)

tree_grid_search <-
  tune_grid(
    tree_wflow_tune,
    resamples = mvb_cvs,
    grid = tree_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

tree_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)
```

## Logistic Regression

```{r}
#| label: logreg-fit

logreg_mod <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

logreg_mod_wflow <- workflow() %>%
  add_model(logreg_mod) %>% 
  add_recipe(recipe_best_subset)

logreg_fit <- logreg_mod_wflow %>%
  fit_resamples(mvb_cvs)

logreg_fit %>%
  collect_metrics()
```

## KNN

```{r}
#| label: KNN-tune

# k_grid <- grid_regular(neighbors(c(1,7000)), 
#                        levels = 71)
# 
# knn_mod_tune <- nearest_neighbor(neighbors = tune()) %>%
#   set_engine("kknn") %>%
#   set_mode("classification")
# 
# results_knn_k <- list_rbind(map2(test_recipes_plus, names(test_recipes_plus), ~find_k(k_grid, mvb_cvs, knn_mod_tune, .x, .y)))
# 
# write.csv(results_knn_k, 'knn_grid_results.csv')

results_knn_k <- read_csv(here('exam_1', 'knn_grid_results.csv'))

results_knn_k %>%
  slice_max(ROC_AUC, n = 10)

results_knn_k %>%
  filter(ROC_AUC > .835) %>%
  ggplot() +
  geom_line(aes(x = k, y = ROC_AUC, color = Recipe)) +
  theme_bw()
```

```{r}
k_grid <- grid_regular(neighbors(c(1,7000)),
                       levels = 701)

knn_mod_tune <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

results_knn_k_2 <- find_k(k_grid, mvb_cvs, knn_mod_tune, recipe_forward_norm, 'forward')

results_knn_k_2 %>%
  slice_max(ROC_AUC, n = 5)

results_knn_k_2 %>%
  ggplot() +
  geom_line(aes(x = k, y = ROC_AUC)) +
  theme_bw()
```

```{r}
k_grid <- grid_regular(neighbors(c(1000,10000)),
                       levels = 10)

knn_mod_tune <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

results_knn_k <- list_rbind(map2(test_recipes_plus[1], names(test_recipes_plus[1]), ~find_k(k_grid, mvb_cvs, knn_mod_tune, .x, .y)))

results_knn_k %>%
  slice_max(ROC_AUC, n = 3)

results_knn_k %>%
  ggplot() +
  geom_line(aes(x = k, y = ROC_AUC, color = Recipe)) +
  theme_bw()
```

```{r}
knn_mod <- nearest_neighbor(neighbors = 10000) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_wflow <- workflow() %>%
  add_model(knn_mod) %>%
  add_recipe(recipe_best_subset)

knn_fit <- knn_wflow %>%
  fit(mvb_train)
```


```{r}
#| label: KNN-tune-2

k_grid <- grid_regular(neighbors(c(15,40)), 
                       levels = 20)

knn_mod_tune <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

results_knn_k_2 <- find_k(k_grid, mvb_cvs, knn_mod_tune, recipe_best_subset_norm, 'best_subset')

results_knn_k_2 %>%
  slice_max(ROC_AUC, n = 5)
```

```{r}
#| label: KNN-tune-3

k_grid <- grid_regular(neighbors(c(20,35)), 
                       levels = 15)

knn_mod_tune <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

results_knn_k_3 <- find_k(k_grid, mvb_cvs, knn_mod_tune, recipe_best_subset_norm, 'best_subset')

results_knn_k_3 %>%
  slice_max(ROC_AUC, n = 3)
```

```{r}
#| label: KNN-fit

knn_mod <- nearest_neighbor(neighbors = 28) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_wflow <- workflow() %>%
  add_model(knn_mod) %>% 
  add_recipe(recipe_best_subset_norm)

knn_fit <- knn_wflow %>%
  fit(mvb_train)
```

## Elastic Net

```{r}
#| label: elastic-net-best-subset-tune

net_grid <- grid_regular(penalty(),
                         mixture(),
                         levels = 100)

net_mod_tune <- logistic_reg(penalty = tune(), 
                             mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

net_wflow_tune <- workflow() %>%
  add_model(net_mod_tune) %>% 
  add_recipe(recipe_forward_norm)

net_grid_search <-
  tune_grid(
    net_wflow_tune,
    resamples = mvb_cvs,
    grid = net_grid,
    metrics = metric_set(accuracy, roc_auc, gain_capture, precision, recall)
  )

net_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)
```

```{r}
#| label: elastic-net-best-subset-tune-2

net_grid <- grid_regular(penalty(c(-5, 1), trans = log2_trans()),
                         mixture(),
                         levels = 101)

net_mod_tune <- logistic_reg(penalty = tune(), 
                             mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

net_wflow_tune <- workflow() %>%
  add_model(net_mod_tune) %>% 
  add_recipe(recipe_forward_norm)

net_grid_search <-
  tune_grid(
    net_wflow_tune,
    resamples = mvb_cvs,
    grid = net_grid,
    metrics = metric_set(accuracy, roc_auc, gain_capture, precision, recall)
  )

net_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 5)
```

```{r}
#| label: elastic-net-best-subset-fit

net_mod <- logistic_reg(penalty = 1.0281138, 
                        mixture = 0.01) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

net_wflow <- workflow() %>%
  add_model(net_mod) %>% 
  add_recipe(recipe_forward_norm)

net_fit <- net_wflow %>%
  fit(mvb_train)
```

## LDA

```{r}
#| label: LDA-tune

lda_mod <- discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification")

results_lda <- list_rbind(map2(test_recipes, names(test_recipes), ~fit_model(mvb_train, mvb_cvs, lda_mod, .x, .y)))

results_lda
```

## QDA

```{r}
#| label: QDA-tune

qda_mod <- discrim_regularized(frac_common_cov = 0) %>% 
  set_engine('klaR') %>% 
  set_mode('classification')

results_qda <- list_rbind(map2(test_recipes[-3], names(test_recipes[-3]), ~fit_model(mvb_train, mvb_cvs, qda_mod, .x, .y)))

results_qda

qda_wflow <- workflow() %>%
  add_model(qda_mod) %>% 
  add_recipe(recipe_best_subset_norm)

qda_fit <- qda_wflow %>%
  fit(mvb_train)
```

# Submitted Models

## Submission 1

```{r}
#| label: RF-upsample-S1

rf_mod <- rand_forest(mtry = 6, 
                      min_n = 67,
                      trees = 1200) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

rf_wflow <- workflow() %>%
  add_model(rf_mod) %>% 
  add_recipe(recipe_full_upsample)

rf_fit <- rf_wflow %>%
  fit(mvb_train)

results <- mvb_test %>%
  mutate(
    ID_Num = id_num,
    Preference = predict(rf_fit, mvb_test, type = "prob")$.pred_1
    ) %>%
  select(ID_Num, Preference)

write_csv(results, 'submission1.csv')

rf_tree_fit_extracted <- rf_fit %>%
  extract_fit_parsnip()

ranger_fit <- rf_tree_fit_extracted$fit

importance(ranger_fit) %>%
  data.frame(Feature = names(.), Importance = .) %>%
  ggplot(aes(x = Importance, y = reorder(Feature, -Importance))) +
  geom_col(fill = 'firebrick') +
  theme_bw() +
  labs(
    x = 'Variable Importance',
    y = 'Features',
    title = 'Random Forest Variable Importance'
  )
```

## Submission 2

```{r}
#| label: Net-S2

net_mod <- logistic_reg(penalty = 1.0281138, 
                        mixture = 0.01) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

net_wflow <- workflow() %>%
  add_model(net_mod) %>% 
  add_recipe(recipe_forward_norm)

net_fit <- net_wflow %>%
  fit(mvb_train)

results <- mvb_test %>%
  mutate(
    ID_Num = id_num,
    Preference = predict(net_fit, mvb_test, type = "prob")$.pred_1
    ) %>%
  dplyr::select(ID_Num, Preference)

write_csv(results, 'submission2.csv')
```





