---
title: "Project Phase 2: Project Proposal"
author: "Lily Cook, Andrew Kerr, Daniel Erro"
format: 
  html:
    code-fold: true
    toc: true
    toc-title: "Outline"
    toc-depth: 4
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: visual
embed-resources: true
---

```{r}
#| label: libraries
#| message: false
#| include: false

library(tidyverse)
library(here)
library(tidymodels)
library(tidyclust)
library(magrittr)
library(readxl)
library(janitor)
library(naniar)
library(rpart)
library(rpart.plot)
library(patchwork)
```

### NOTE

We are switching our primary and secondary goals for this data analysis project.

-   New Primary Goal: Predict what a college's ranking is based on various predictor variables with a focus on interpretability.
-   New Secondary Goal: Cluster colleges together based on various predictor variables.

# Project Proposal

[Organization: Fair Opportunity Project](https://www.fairopportunityproject.org/)

About FOP from their website: Fair Opportunity Project (Fair Opp) is a nationally-recognized, federally-funded education nonprofit. We aim to help students attend and afford college, while also exploring other opportunities post-high school.

Every year, Fair Opp sends free college application advice to over 63,000 public educators via our guide. We also have over 100 college and financial aid mentors who help students maximize their aid, put together winning applications, and file the FAFSA.

Fair Opp has been featured in Forbes 30 Under 30, the New York Times, and the Harvard Education Magazine.

We are able to provide these resources through private and public donors, and we are happy to say they are all free.

## Introduction

The project focuses on helping students decide what college is best for them and making this decision more accessible. By predicting the rankings, students will have a better idea of how their school of choice will continue to perform throughout their education. This will assist your organization in the goal to help students attend college, as finding the right college is a vital part of the process of deciding to go to college. If students have a good idea of what schools will be right for them, then they are more likely to not only decide to go to college, but also stay in college. The project has the main goal of predicting what a college's ranking is based on various predictor variables and the secondary goal of clustering colleges together based on various predictor variables.

The main dataset used for this analysis comes from the U.S. Department of Education. The government creates this dataset to provide students and educators with a comprehensive and accessible tool for evaluating and comparing U.S. colleges. This dataset contains numerous variables from over 6,000 colleges, sourced from federally reported information from the institutions. The dataset on college rankings for this project comes from US News & World Report, which generally is seen as a reliable source for college rankings and was compiled by Andrew Reiter. US News & World Report is a journalism company that creates these rankings to inform the general public, especially prospective college students. Ranks are determined based on various metrics including graduation rates, student debt, and student to faculty ratio to calculate the rankings. These ranks will be our target variable of our analysis. Both sets collect data on the college level. To prepare for this analysis, the two datasets will be joined and all colleges that are not featured in the rankings set will be dropped, leaving 188 total colleges in our analysis.

This project will assist college-bound students in their decision of which college to attend. Additionally, this will hopefully assist students in determining how they will fund their attendance, as they will be able to see what type and amount of loan they may need to take out, along with the amount of money they can be expected to earn once graduating, ensuring that they pick a college that will not trap them in debt. This project could be biased against community colleges, as we are not including them in our analyses, but are still a viable option for students. This project will have minimal privacy and bias concerns, as all information is federally reported information about the schools as a whole, rather than individual students. To ensure ethical work, this project will be fully transparent on all methods, analyses and data sources.

## Previous Work

<https://www.usnews.com/education/best-colleges/articles/how-us-news-calculated-the-rankings>

The U.S. News and World Report rank colleges based on 17 factors. This organization applies weights to these created factors based on how important they believe them to be. The similarity between our project and the U.S. News and World Report is that we are ranking colleges in the U.S. based on similar data, however our project will not be creating factors and weighing them, but using the raw data itself to generate the rankings. Additionally, we plan to determine what colleges are similar to each other through clustering, which is not something this organization does.

## Exploratory Analysis

### College Rankings

```{r}
#| label: rankings-read-in
#| message: false

rank <- read_xlsx(here('project', 
                       'data', 
                       'US-News-National-University-Rankings-Top-150-Through-2025.xlsx')) %>%
  clean_names()
```

```{r}
#| label: rankings-cleaning

rank %>%
  summarise(across(everything(), ~ round(sum(is.na(.)) / nrow(rank), 2))) %>%
  pivot_longer(cols = everything(), 
               names_to = 'Column', 
               values_to = 'Proportion_Missing') %>%
  arrange(-Proportion_Missing)

rank %>%
  select(-university_name, -state, -ipeds) %>%
  summarise(across(everything(), ~ max(., na.rm = T))) %>%
  pivot_longer(cols = everything(), 
               names_to = 'year', 
               values_to = 'max_rank') %>%
  mutate(year = as.numeric(gsub("x", "", year))) %>%
  ggplot() +
  geom_col(aes(x = year, y = max_rank), fill = 'cornflowerblue') +
  theme_bw() +
  labs(x = "Year",
       y = "Maximum Ranking",
       title = "Maximum Ranking by Year")
```

From the above table, we notice that the ranking for many colleges each year is NA, meaning that they were not ranked within the top 150 colleges that year. Additionally, each year the top 150 colleges were not ranked, but less. The histogram shows that in the 1900s, only the top 25 colleges were ranked, then in the 1996 this increased to the top 50. In 2004 the top 125 colleges were ranked, and this amount increased to 150 in 2011. The reason the maximum ranking is not 50, 100, or 150 exactly is because the metric used to rank these colleges could tie, in which case the tied colleges were assigned the same rank.

Since most students do not differentiate between colleges close in ranking, we decided to bin the ranks into 5 bins: 1-25, 26-50, 51-75, 75-100, and 100+. We will only use ranking from 2004-2025 since these years ranked at least the top 100 colleges, and we will drop any colleges that have never been in the top 100 within these years.

```{r}
#| label: long-rank

rank <- rank %>%
  pivot_longer(
    cols = starts_with("x"),
    names_to = "year",
    names_prefix = "x", 
    values_to = "rank"
  ) %>%
  mutate(
    year = as.numeric(year),
    rank_bin = factor(case_when(
      rank >= 1 & rank <= 25 ~ "1-25",
      rank >= 26 & rank <= 50 ~ "26-50",
      rank >= 51 & rank <= 75 ~ "51-75",
      rank >= 76 & rank <= 100 ~ "76-100",
      TRUE ~ "100+"
    ), levels = c("1-25", "26-50", "51-75", "76-100", "100+"))
    ) %>%
  filter(year >= 2004) %>%
  group_by(ipeds) %>%
  filter(any(rank <= 100)) %>%
  ungroup() %>%
  select(university_name, ipeds, year, rank_bin)

rank %>%
  ggplot(aes(x = rank_bin)) +
  geom_bar(color = "black", fill = "cornflowerblue") +
  theme_bw() +
  facet_wrap(~ year, ncol = 6) +
  labs(x = "Rank",
       y = "Count",
       title = "Colleges in each Binnned Rank by Year")
```

### College Scorecard

```{r}
#| label: scorecard-read-in
#| message: false
#| eval: false

card_02_03 <- read_csv(here('project', 'data', 'MERGED2002_03_PP.csv'))
card_03_04 <- read_csv(here('project', 'data', 'MERGED2003_04_PP.csv'))
card_04_05 <- read_csv(here('project', 'data', 'MERGED2004_05_PP.csv'))
card_05_06 <- read_csv(here('project', 'data', 'MERGED2005_06_PP.csv'))
card_06_07 <- read_csv(here('project', 'data', 'MERGED2006_07_PP.csv'))
card_07_08 <- read_csv(here('project', 'data', 'MERGED2007_08_PP.csv'))
card_08_09 <- read_csv(here('project', 'data', 'MERGED2008_09_PP.csv'))
card_09_10 <- read_csv(here('project', 'data', 'MERGED2009_10_PP.csv'))
card_10_11 <- read_csv(here('project', 'data', 'MERGED2010_11_PP.csv'))
card_11_12 <- read_csv(here('project', 'data', 'MERGED2011_12_PP.csv'))
card_12_13 <- read_csv(here('project', 'data', 'MERGED2012_13_PP.csv'))
card_13_14 <- read_csv(here('project', 'data', 'MERGED2013_14_PP.csv'))
card_14_15 <- read_csv(here('project', 'data', 'MERGED2014_15_PP.csv'))
card_15_16 <- read_csv(here('project', 'data', 'MERGED2015_16_PP.csv'))
card_16_17 <- read_csv(here('project', 'data', 'MERGED2016_17_PP.csv'))
card_17_18 <- read_csv(here('project', 'data', 'MERGED2017_18_PP.csv'))
card_18_19 <- read_csv(here('project', 'data', 'MERGED2018_19_PP.csv'))
card_19_20 <- read_csv(here('project', 'data', 'MERGED2019_20_PP.csv'))
card_20_21 <- read_csv(here('project', 'data', 'MERGED2020_21_PP.csv'))
card_21_22 <- read_csv(here('project', 'data', 'MERGED2021_22_PP.csv'))
card_22_23 <- read_csv(here('project', 'data', 'MERGED2022_23_PP.csv'))
card_23_24 <- read_csv(here('project', 'data', 'Most-Recent-Cohorts-Institution.csv'))
```

```{r}
#| label: card-cleaning
#| eval: false

prop_na <- card_23_24 %>%
  summarise(across(everything(), ~ round(sum(is.na(.)) / nrow(card_23_24), 2))) %>%
  pivot_longer(cols = everything(), 
               names_to = 'Column', 
               values_to = 'Proportion_Missing') %>%
  arrange(-Proportion_Missing)

prop_na
```

We need scorecard data from past years to train our model on past years rankings. Since the Department of Education only supplies merged scorecards of past years, we will use the data for a given year and its following year to predict the next years ranking. For example, the rankings from 2006 will use the merged data from 2004 to 2005. For clustering, we will only use the most recent data from 2023 to 2024. 

NOTES:

-   What year variable values are from: CollegeScorecardDataDirectory \> Most_Recent_Inst_Cohort_Map

-   What each variable means: [Technical Documentation: College Scorecard Institution-Level Data](https://collegescorecard.ed.gov/assets/InstitutionDataDocumentation.pdf)

The 3,305 variables can be grouped into the following groupings:

-   ABOUT INSTITUTION
    -   Institution identifiers, location, degree type and profile, programs offered, and the academic profile of students enrolled
-   ACADEMICS
    -   Types of academic offerings available at each institution
-   ADMISSIONS
    -   Information that describes the admissions rate and SAT/ACT scores of students
-   COSTS
    -   Information about the costs to students of an institution
-   STUDENTS AND STAFF
    -   Demographic and other details about the student body and the staff of the institution
-   FINANCIAL AID
    -   Amount of debt that students can expect to borrow, and the loan performance of former students
-   COMPLETION AND RETENTION
    -   Outcomes such as students who found a job and successfully repayed student loans
-   OUTCOMES FOR TITLE IV STUDENTS
    -   Completion and transfer rate for students who receive federal financial aid
-   EARNINGS
    -   Earnings and employment prospects of former students
-   REPAYMENT
    -   Debt burden of attending college and the loan performance metrics for each institution

There are too many to go through individually, so we will rely on LASSO Regression to narrow down predictors for prediction and PCA for clustering. Additionally, many of the columns solely consist of NA values or consist of a majority NA values. Therefore, we will drop any columns where at least 40% of the observations are NA. For consistency, we will drop these same columns across all years.

```{r}
#| label: drop-NA-cols
#| eval: false

remove_cols <- prop_na %>%
  filter(Proportion_Missing >= 0.4) %>%
  pull(Column)

remove_cols_check <- prop_na %>%
  filter(Proportion_Missing >= 0.4, Proportion_Missing <= 0.5) %>%
  pull(Column)

joined_data_23_24 <- card_23_24 %>% 
  mutate(year = 2025) %>%
  right_join(rank, by = join_by(UNITID == ipeds, year == year)) 

joined_data_23_24 %>%
  select(university_name, all_of(remove_cols_check)) %>%
  gg_miss_fct(x = ., fct = university_name)
  
joined_data_23_24 %>%
  select(starts_with("N"))

joined_data_23_24 %>%
  filter(year == 2025) %>%
  group_by(CONTROL_PEPS) %>%
  count()

card_02_03 <- card_02_03 %>% select(-all_of(remove_cols)) %>% mutate(year = 2004)
card_03_04 <- card_03_04 %>% select(-all_of(remove_cols)) %>% mutate(year = 2005)
card_04_05 <- card_04_05 %>% select(-all_of(remove_cols)) %>% mutate(year = 2006)
card_05_06 <- card_05_06 %>% select(-all_of(remove_cols)) %>% mutate(year = 2007)
card_06_07 <- card_06_07 %>% select(-all_of(remove_cols)) %>% mutate(year = 2008)
card_07_08 <- card_07_08 %>% select(-all_of(remove_cols)) %>% mutate(year = 2009)
card_08_09 <- card_08_09 %>% select(-all_of(remove_cols)) %>% mutate(year = 2010) 
card_09_10 <- card_09_10 %>% select(-all_of(remove_cols)) %>% mutate(year = 2011)
card_10_11 <- card_10_11 %>% select(-all_of(remove_cols)) %>% mutate(year = 2012)
card_11_12 <- card_11_12 %>% select(-all_of(remove_cols)) %>% mutate(year = 2013)
card_12_13 <- card_12_13 %>% select(-all_of(remove_cols)) %>% mutate(year = 2014)
card_13_14 <- card_13_14 %>% select(-all_of(remove_cols)) %>% mutate(year = 2015) 
card_14_15 <- card_14_15 %>% select(-all_of(remove_cols)) %>% mutate(year = 2016)
card_15_16 <- card_15_16 %>% select(-all_of(remove_cols)) %>% mutate(year = 2017)
card_16_17 <- card_16_17 %>% select(-all_of(remove_cols)) %>% mutate(year = 2018)
card_17_18 <- card_17_18 %>% select(-all_of(remove_cols)) %>% mutate(year = 2019)
card_18_19 <- card_18_19 %>% select(-all_of(remove_cols)) %>% mutate(year = 2020)
card_19_20 <- card_19_20 %>% select(-all_of(remove_cols)) %>% mutate(year = 2021)
card_20_21 <- card_20_21 %>% select(-all_of(remove_cols)) %>% mutate(year = 2022)
card_21_22 <- card_21_22 %>% select(-all_of(remove_cols)) %>% mutate(year = 2023)
card_22_23 <- card_22_23 %>% select(-all_of(remove_cols)) %>% mutate(year = 2024)  
card_23_24 <- card_23_24 %>% select(-all_of(remove_cols)) %>% mutate(year = 2025)
```

The columns with 40-50% NA values are Net Costs for a small subset of Private Colleges. Since there are other variables that measure similar metrics, we decided to drop these columns.

### Joined Data

```{r}
#| label: check-join
#| eval: false

r_lst <- rank %>%
  select(ipeds) %>%
  unique()

c_lst <- card_23_24 %>%
  select(UNITID) %>%
  unique()

nrow(r_lst)
nrow(c_lst)
nrow(inner_join(r_lst, c_lst, by = join_by(ipeds == UNITID)))

anti_join(r_lst, c_lst, by = join_by(ipeds == UNITID))

# Drop this observation because it is a branch of NYU that only existed separately for 3 years, and NYU is already in the dataset
rank %>%
  filter(ipeds == 194541)

card_all <- rbind(card_02_03, card_03_04, card_04_05, card_05_06, card_06_07, 
                  card_07_08, card_08_09, card_09_10, card_10_11, card_11_12, 
                  card_12_13, card_13_14, card_14_15, card_15_16, card_16_17, 
                  card_17_18, card_18_19, card_19_20, card_20_21, card_21_22, 
                  card_22_23, card_23_24)

joined_data <- left_join(rank, card_all, by = join_by(ipeds == UNITID, year == year))
```

```{r}
#| label: final-cleaning
#| eval: false

missing_obs <- joined_data %>%
  filter(is.na(INSTNM))

missing_obs %>%
  group_by(ipeds, year) %>%
  summarise(across(everything(), ~ sum(is.na(.))))

joined_data <- joined_data %>%
  anti_join(missing_obs, by = join_by(ipeds == ipeds, year == year))
```

15 rows of data have no data in the scorecards data for any variables, so they will be dropped. 

```{r}
#| label: save-data
#| eval: false

write_csv(joined_data, "joined_data.csv")
```

## Preliminary Results

### Data 

```{r}
#| label: read-in-data
#| message: false
#| warning: false

joined_data <- read_csv(here('project', 'joined_data.csv'))
```

### Impute Means

```{r}
#| label: means

overall_means <- joined_data %>%
  summarise(across(where(is.numeric), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}"))

df_imputed <- joined_data %>%
  group_by(ipeds) %>%
  mutate(
    across(
      where(is.numeric),
      ~ if_else(
        is.na(.x),
        coalesce(mean(.x, na.rm = TRUE), overall_means[[paste0("mean_", cur_column())]]),
        .x
      ),
      .names = "{.col}"
    )
  ) %>%
  ungroup()

df_imputed %>%
  summarise(across(where(is.numeric), ~ round(sum(is.na(.)) / nrow(df_imputed), 2))) %>%
  pivot_longer(cols = everything(), 
               names_to = 'Column', 
               values_to = 'Proportion_Missing') %>%
  arrange(-Proportion_Missing)
```

For simplicity, we will only use numeric columns.

### Functions

```{r}
#| label: test-k-function

fit_kmeans <- function(data, recipe, k, counts = F) {

  km_spec <- k_means(num_clusters = k)
  
  km_wflow <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(km_spec)
  
  km_fitted <- km_wflow %>% fit(data)

  engine_fit <- km_fitted %>% 
    extract_fit_engine()
  
  df_results <- data.frame(
    k = k,
    Cluster = paste0("Cluster_", seq(1:k)),
    Withiness = engine_fit$withinss,
    Betweenss = engine_fit$betweenss,
    Total_Withiness = engine_fit$tot.withinss
  ) %>% pivot_wider(names_from = Cluster, values_from = Withiness)
  
  if(counts) {
    
    df_counts <- data.frame(
      k = k,
      extract_cluster_assignment(km_fitted) %>%
        group_by(.cluster) %>%
        count()
    ) %>% pivot_wider(names_from = .cluster, values_from = n)
    
    return(list(results = df_results, counts = df_counts))
    
  }
    
  return(results = df_results)
  
}
```

### Starting Recipes

```{r}
#| label: recipes

base_recipe <- recipe(rank_bin ~ ., data = df_imputed) %>%
  update_role(university_name, new_role = "id") %>%
  step_rm(ipeds, year, INSTNM, ADDR, CITY, STABBR, ZIP, LATITUDE, LONGITUDE, 
          OPEID, INSTURL, NPCURL, MAIN, NUMBRANCH, OPEFLAG, ACCREDAGENCY, 
          ACCREDCODE, HIGHDEG, ICLEVEL, PREDDEG, SCH_DEG, CONTROL_PEPS, CCBASIC,
          CCUGPROF, CCSIZSET, DISTANCEONLY, CIP01ASSOC, HCM2) %>%
  step_rm(all_nominal_predictors())

base_recipe_norm <- base_recipe %>%
  step_zv(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors())
```

### LASSO Regression (Variable Reduction)

```{r}
#| label: tuning-lasso
#| eval: false

joined_data_cvs <- vfold_cv(df_imputed, v = 10)

lasso_grid <- grid_regular(penalty(c(-15, -5), 
                                   trans = log2_trans()), 
                           levels = 10)

lasso_mod_tune <- multinom_reg(penalty = tune(), 
                               mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

lasso_wflow_tune <- workflow() %>%
  add_model(lasso_mod_tune) %>% 
  add_recipe(base_recipe_norm)

lasso_grid_search <-
  tune_grid(
    lasso_wflow_tune,
    resamples = joined_data_cvs,
    grid = lasso_grid,
    metrics = metric_set(accuracy, precision, recall)
  )

lasso_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  arrange(-mean)
```

```{r}
#| label: lasso_fit

lasso_mod <- multinom_reg(penalty = 6.644482e-04, 
                          mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

lasso_wflow <- workflow() %>%
  add_model(lasso_mod) %>% 
  add_recipe(base_recipe_norm)

lasso_fit <- lasso_wflow %>%
  fit(df_imputed)

lasso_output <- lasso_fit %>%
  extract_fit_parsnip() %>%
  tidy(n = Inf) %>%
  filter(estimate != 0)

lasso_output %>%
  filter(term != "(Intercept)") %>%
  arrange(-abs(estimate))

keep_vars <- lasso_output %>%
  filter(term != "(Intercept)") %>%
  pull(term) %>%
  unique()

keep_vars %>%
  length()

df_imputed_reduced <- df_imputed %>%
  select(ipeds, university_name, year, rank_bin, all_of(keep_vars))
```

To reduce the amount of variables, we ran LASSO Regression to zero-out the less important variables. The most influential predictor is the amount of money the 90th percentile of graduates are earning after 10 years, followed by the percentage of undergraduate students who received federal loans and the percentage of undergraduate African American students.

### Recipes

```{r}
#| label: new-recipes

base_recipe <- recipe(rank_bin ~ ., data = df_imputed_reduced) %>%
  update_role(university_name, new_role = "id") %>%
  step_rm(ipeds, year) %>%
  step_rm(all_nominal_predictors())

base_recipe_norm <- base_recipe %>%
  step_zv(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors())

cluster_recipe_pca <- recipe(~., data = df_imputed_reduced) %>%
  update_role(university_name, new_role = "id") %>%
  step_rm(ipeds, year, rank_bin) %>%
  step_rm(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_pca(all_numeric(), threshold = 0.8)
```

### Decision Tree

```{r}
#| label: dc-tune

set.seed(123)

df_imputed_reduced_cvs <- vfold_cv(df_imputed_reduced, v = 10)

tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          min_n(),
                          levels = 5)

tree_mod_tune <- decision_tree(cost_complexity = tune(),
                               tree_depth = tune(),
                               min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_wflow_tune <- workflow() %>%
  add_model(tree_mod_tune) %>% 
  add_recipe(base_recipe)

tree_grid_search <-
  tune_grid(
    tree_wflow_tune,
    resamples = df_imputed_reduced_cvs,
    grid = tree_grid,
    metrics = metric_set(accuracy, roc_auc, precision, recall, gain_capture)
  )

tree_grid_search %>% 
  collect_metrics() %>%
  filter(.metric == "gain_capture") %>%
  slice_max(mean, n = 10)
```
```{r}
#| label: dc-fit

tree_mod <- decision_tree(cost_complexity = 1.000000e-10,
                          tree_depth = 15,
                          min_n = 21) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_wflow <- workflow() %>%
  add_model(tree_mod) %>% 
  add_recipe(base_recipe)

tree_fit <- tree_wflow %>% fit(df_imputed_reduced)

tree_fitted <- tree_fit %>% 
  extract_fit_parsnip()

rpart.plot(tree_fitted$fit, roundint = FALSE)

tree_mod <- decision_tree(cost_complexity = 1.000000e-10,
                          tree_depth = 3,
                          min_n = 21) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_wflow <- workflow() %>%
  add_model(tree_mod) %>% 
  add_recipe(base_recipe)

tree_fit <- tree_wflow %>% fit(df_imputed_reduced)

tree_fitted <- tree_fit %>% 
  extract_fit_parsnip()

rpart.plot(tree_fitted$fit, roundint = FALSE)
```

### K-Means

```{r}
#| label: kmeans
 
set.seed(123)

# k_lst <- seq(2, 15, 2)
# 
# kmeans_results <- list_rbind(map(k_lst, ~fit_kmeans(df_imputed_reduced, cluster_recipe_pca, .x)))
# 
# kmeans_results

km_spec <- k_means(num_clusters = 5)

km_wflow <- workflow() %>%
  add_recipe(cluster_recipe_pca) %>%
  add_model(km_spec)

km_fitted <- km_wflow %>% fit(df_imputed_reduced)

engine_fit <- km_fitted %>% 
  extract_fit_engine()

################################################################################

pca_output <- cluster_recipe_pca %>%
  prep() %$%
  steps %>% 
  pluck(5) %$%
  res

PC1 <- pca_output$rotation[, 1]
PC2 <- pca_output$rotation[, 2]

pc1_df <- data.frame(Variable = names(PC1), PC1 = PC1)

pc1_df %>%
  arrange(-abs(PC1))

ggplot(pc1_df, aes(x = Variable, y = 1, fill = PC1)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
  labs(title = "Heatmap of PC1", x = "Variables", y = "")

################################################################################

cluster_means <- as.data.frame(df_imputed_reduced) %>%
  mutate(
    cluster = extract_cluster_assignment(engine_fit)$.cluster
  ) %>%
  group_by(cluster) %>%
  summarize(
    across(.cols = PCIP04:MD_EARN_WNE_MALE1_P9, 
           .fns = ~ mean(.x)
           )
    )

cluster_means_long <- cluster_means %>%
  pivot_longer(cols = PCIP04:MD_EARN_WNE_MALE1_P9, names_to = 'Feature', values_to = 'Mean')

cluster_means_long %>%
  ggplot(aes(x = Feature, y = cluster, fill = Mean)) +
    geom_tile() +
    scale_fill_viridis_c() +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 30, hjust = 1), 
          axis.title.x = element_blank()) +
    labs(title = 'Feature Mean Values by Cluster',
         y = 'Cluster',
         fill = 'Scaled Mean')

################################################################################

cluster_trained <- cluster_recipe_pca %>% 
  prep(df_imputed_reduced)

cluster_pcs <- cluster_trained %>% 
  bake(df_imputed_reduced)

################################################################################

plot1 <- cluster_pcs %>%
  mutate(extract_cluster_assignment(engine_fit),
         rank_bin = df_imputed_reduced$rank_bin) %>%
  ggplot(aes(x = PC02, y = PC01, color = rank_bin)) +
  geom_point() +
  theme_bw() +
  labs(title = "College Clusters in PC1 by PC2 and PC2 by PC3", color = "Binned Rank") +
  theme(axis.title.x = element_blank())

plot2 <- cluster_pcs %>%
  mutate(extract_cluster_assignment(engine_fit),
         rank_bin = df_imputed_reduced$rank_bin) %>%
  ggplot(aes(x = PC02, y = PC03, color = rank_bin)) +
  geom_point() +
  theme_bw() +
  labs(color = "Binned Rank") 

combined_plot <- plot1 + plot2 + 
  plot_layout(ncol = 1, guides = "collect") & 
  theme(legend.position = "right")

combined_plot

################################################################################

plot1 <- cluster_pcs %>%
  mutate(extract_cluster_assignment(engine_fit),
         rank_bin = df_imputed_reduced$rank_bin) %>%
  ggplot(aes(x = PC02, y = PC01, color = .cluster)) +
  geom_point() +
  theme_bw() +
  labs(title = "College Clusters in PC1 by PC2 and PC2 by PC3", color = "Cluster") +
  theme(axis.title.x = element_blank())

plot2 <- cluster_pcs %>%
  mutate(extract_cluster_assignment(engine_fit),
         rank_bin = df_imputed_reduced$rank_bin) %>%
  ggplot(aes(x = PC02, y = PC03, color = .cluster)) +
  geom_point() +
  theme_bw() +
  labs(color = "Cluster") 

combined_plot <- plot1 + plot2 + 
  plot_layout(ncol = 1, guides = "collect") & 
  theme(legend.position = "right")

combined_plot
```

Above we see that 

## Project Timeline and Goals

The conclusion we expect to be able to make at the end of the project is what predictors are the most important in determining higher rankings for colleges. We intend to use ridge regression to narrow down our predictors, followed by a tree-based model for predictions. To cluster the colleges, we will use K-means after pre-processing with principle component analysis.

# Reflection
## Goals
### What is the main takeaway message you hope to be able to supply to the partner?

The main takeaway message we hope to be able to supply to the partner is what predictors are the most important in determining higher rankings for colleges.

### What types of summaries or visualizations do you think would be impactful to communicate your results?

The average ranking and highest ranking for each college would be impactful summaries that are easy to understand and interpret. We can also plot the two most important principle components that we use for clustering the colleges, and color each observation on the plot (each college) by which cluster they are included in, which would provide a helpful visualization for our clusters.

### What are some conclusions that you would be excited to discover? What results might be a bit disappointing to you?

We would be excited to discover that our final model provides accurate college rankings according to US News and World Report. We would also be excited to discover that many of the colleges in our dataset can clearly be clustered into different groups. However, we recognize that the clusters may not be so easily separable, which would be disappointing. It would also be a bit disappointing to us if we can not accurately predict college rankings based on the predictor variables in our final model.

## Challenges

### Are there any aspects of the data that you are worried about, or that are challenging to clean / manipulate?

Our data includes a large amount of N/A values, since some colleges are not ranked in the top 150 every year, so we had to find a way to provide these unranked colleges some sort of rank value each year. Furthermore, our data set also includes some years where less than 150 colleges are ranked, and some years where more than 150 colleges are ranked. So there are some inconsistencies with how colleges are ranked each year.

### Is there any information that is not in your dataset that you wish you could have?

The U.S. News and World Report U.S. college rankings started in 1984, and they only ranked at most the top 25 colleges until 1996, where they began ranking the top 50. In 2004, they began ranking the top 120 colleges. And finally, in 2011, they started ranking the top 150 colleges in the U.S. Ideally, we would have data on the top 150 U.S. colleges all the way back in 1984, but the top 150 rankings did not start until 2011.

### Are there any other concerns you have regarding the success of your project?

Our data set has over 3000 columns, so parsing through each column to find the significant variables may be difficult. A concern we have is that we may miss some important predictors to include in our final model based on the fact that we have so many potential predictors to choose from.
